# **计网总结**

## 1，OSI，TCP/IP模型

#### 一、OSI七层模型

OSI七层协议模型主要是：应用层（Application）、表示层（Presentation）、会话层（Session）、传输层（Transport）、网络层（Network）、数据链路层（Data Link）、物理层（Physical）。

#### 二、TCP/IP四层模型

TCP/IP是一个四层的体系结构，主要包括：应用层、运输层、网际层和网络接口层。从实质上讲，只有上边三层，网络接口层没有什么具体的内容。

![TCP/IP体系结构](http://img.blog.csdn.net/20170822224422517?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU2lsZW5jZU9P/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170822224527532?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU2lsZW5jZU9P/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 三、五层体系结构

五层体系结构包括：应用层、运输层、网络层、数据链路层和物理层。 
五层协议只是OSI和TCP/IP的综合，实际应用还是TCP/IP的四层结构。为了方便可以把下两层称为网络接口层。

三种模型结构： 
![这里写图片描述](http://img.blog.csdn.net/20170822222325781?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU2lsZW5jZU9P/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![这里写图片描述](http://img.blog.csdn.net/20170822224933262?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvU2lsZW5jZU9P/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 四、各层的作用

##### 1、物理层：

**主要定义物理设备标准**，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输,到达目的地后在转化为1、0，也就是我们常说的数模转换与模数转换）。这一层的数据叫做**比特**。 　　

##### 2、数据链路层：

**定义了如何让格式化数据以进行传输**，以及如何让控制对物理介质的访问。这一层通常还提供错误检测和纠正，以确保数据的可靠传输。 　　

##### 3、网络层：              

**在位于不同地理位置的网络中的两个主机系统之间提供连接和路径选择**。Internet的发展使得从世界各站点访问信息的用户数大大增加，而网络层正是管理这种连接的层。 　　

##### 4、运输层：

**定义了一些传输数据的协议和端口号**（WWW端口80等），如： 
**TCP**（transmission control protocol –传输控制协议，传输效率低，可靠性强，用于传输可靠性要求高，数据量大的数据） 
**UDP**（user datagram protocol–用户数据报协议，与TCP特性恰恰相反，用于传输可靠性要求不高，数据量小的数据，如QQ聊天数据就是通过这种方式传输的）。 主要是将从下层接收的数据进行分段和传输，到达目的地址后再进行重组。常常把这一层数据叫做段。 　　

##### 5、会话层：                                                                                                                                                                                                                              

**通过运输层（端口号：传输端口与接收端口）建立数据传输的通路**。主要在你的系统之间发起会话或者接受会话请求（设备之间需要互相认识可以是IP也可以是MAC或者是主机名） 　　

##### 6、表示层：

**可确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取**。例如，PC程序与另一台计算机进行通信，其中一台计算机使用扩展二一十进制交换码（EBCDIC），而另一台则使用美国信息交换标准码（ASCII）来表示相同的字符。**如有必要，表示层会通过使用一种通格式来实现多种数据格式之间的转换。** 　　

##### 7、应用层：

是最靠近用户的OSI层。这一层为用户的应用程序（例如电子邮件、文件传输和终端仿真）提供网络服务。

## 2,数据链路层和运输层的流量控制以及差错控                                                                                                                                                                                                                                                                                                                          制

#### ① 停止等待，后退N帧和滑动窗口

>  【1】这三种协议是做什么的？
>
>    流量控制：接收方控制发送方，发送方的速率不要太快，让接收方来得及接收。
>    
>    差错控制：帧错误、帧丢失、帧重复。
>
>  

 【2】无图无真相

 ![差错控制](https://img-blog.csdn.net/20150613111307063) 

#### ②数据链路层的差错控制和运输层的可靠传输有什么区别？

- 从“干什么”的角度来讲

  - 数据链路层负责结点之间链路的事情。把有比特查错的物理信道变成无比特差错的数据链路。
  - 运输层负责应用进程之间端到端的事情。就两项任务：差错管理+业务复用。

- 从“服务”的角度来讲
  **当然是为上一层服务啦！**

  - 数据链路层将源机网络层来的数据可靠地传输到相邻结点的目标机网络层。
  - 传输层为应用进程提供可靠的，无误的数据传输，屏蔽下面网络核心的细节。

- 从“怎么干”的角度来讲

  - 数据链路层主要靠三大法宝：
    封装成帧：“信封”。
    透明传输：帧中可以有控制字。
    差错检测：FCS，CRC循环冗余检验。
  - 运输层（实际上就是TCP协议）主要靠：
    面向连接的服务。
    停止等待、后退N帧、滑动窗口。

- 从“干到什么程度”来讲

  - 数据链路层可以做到：“无比特错误”
    “无差错接收”：“凡是接收端数据链路层接受的帧均无差错”.
    不保证“可靠传输”：只考虑“帧错误”，不考虑“帧丢失”，“帧重复”和“帧失序”。

    - **注意1**现在的数据链路层协议：
      对通信质量良好的有线信道：不采用确认重传
      对通信质量不好的无线信道：确认重传

    - **注意2**

      PPP协议没有确认重传机制。（以太网有木有我也不知道。。。欢迎大侠指正）

  - TCP协议是可靠传输的最后一道屏障，必须真正做到“可靠传输”。
    “可靠传输”：帧错误、帧丢失、帧重复、帧失序。
    IP是“尽最大努力交付”，不可靠传输，所以TCP要采用确认重传机制，就是1）里讲的停止等待、后退N帧和滑动窗口

## 3,流量控制



### 3.1  TCP协议总结

前言:在学习tcp三次握手的过程之中,由于一直无法解释tcpdump命令抓的包中seq和ack的含义,就将tcp协议往深入的了解了一下,了解到了几个协议,做一个小结.

先来看看我的问题:
这是用tcpdump命令抓的三次握手的包,可以看到seq和ack都比较大,我自己也无法解释原因.


第二张是在同一过程中用Wireshark抓的包,其中seq和ack还比较正常,难道原因就是我不懂tcpdump命令中的数据?我的解释是Wireshark和tcpdump中抓的包,数据的显示方式可能不同,最后学长说可以用 -S 将tcp的序列号以绝对值形式输出，而不是相对值。转化了一下,第三次的ack就正常了.主要还是得知道seq和ack一样,都是字节序列号,一共4个字节,范围都是从[0,2^32 - 1].

$ tcpdump -S port 80



#### 一:停止等待协议

停止等待协议是tcp保证传输可靠的重要途径,”停止等待”就是指发送完一个分组就停止发送,等待对方的确认,只有对方确认过,才发送下一个分组.

1:**无差错情况**:发送方发送分组,接收方在规定时间内收到,**并且回复确认**.发送方再次发送……

**2:超时重传有以下三种情况:**
(1)**分组丢失**:发送方发送分组,接收方没有收到分组,那么接收方不会发出确认,只要发送方过一段时间没有收到确认,就认为刚才的分组丢了,那么发送方就会再次发送.
(2):**确认丢失**:发送方发送成功,接收方接收成功,确认分组也被发送,但是分组丢失,那么到了等待时间,发送方没有收到确认,又会发送分组过去,此时接收方前面已经收到了分组,那么此时接收方要做的事就是:丢弃分组,重新发送确认.
(3):**传送延迟**:发送方发送成功,接收方接收成功,确认分组也被发送,没有丢失,但是由于传输太慢,等到了发送方设置的时间,发送方又会重新发送分组,此时接收方要做的事情:丢弃分组,重新发送确认. 发送方如果收到两个或者多个确认,就停止发送,丢弃其他确认.****

**停止等待协议的优点是简单,但是缺点是信道的利用率太低,一次发送一条消息,使得信道的大部分时间内都是空闲的,为了提高效率,我们采用流水线传输**,这就与下面两个协议有关系了.

#### 二:连续ARQ协议和滑动窗口协议

**这两个协议主要解决的问题信道效率低和增大了吞吐量,以及控制流量的作用.**

**连续ARQ协议:**它是指发送方维护着一个窗口,这个窗口中不止一个分组,有好几个分组,窗口的大小是由接收方返回的win值决定的,所以窗口的大小是动态变化的,只要在窗口中的分组都可以被发送,这就使得TCP一次不是只发送一个分组了,从而大大提高了信道的利用率.并且它采用累积确认的方式,对于按序到达的最后一个分组发送确认.
**滑动窗口协议:**之所以叫滑动窗口协议,是因为窗口是不断向前走的,该协议允许发送方在停止并等待确认前发送多个数据分组。由于发送方不必每发一个分组就停下来等待确认，因此该协议可以加速数据的传输,还可以控制流量的问题.

**累积确认**:如果发送方发送了5个分组,接收方只收到了1,2,4,5,没有收到3分组,那么我的确认信息只会说我期望下一个收到的分组是第三个,此时发送方会将3,4,5,全部重发一次,当通信质量不是很好的时候,连续ARQ还是会带来负面影响.



### 3.2	TCP流量控制

#### 1.TCP的滑动窗口

​    为了提高信道的利用率TCP协议不使用停止等待协议，而是使用连续ARQ协议，意思就是**可以连续发出若干个分组然后等待确认，而不是发送一个分组就停止并等待该分组的确认**。

​    TCP的两端都有发送/接收缓存和发送/接收窗口。TCP的缓存是一个循环队列，其中发送窗口可以用3个指针表示。而发送窗口的大小受TCP数据报中窗口大小的影响，TCP数据报中的窗口大小是接收端通知发送端其还可以接收多少数据，所以**发送窗口根据接收的的窗口大小的值动态变化**。

​    以下的几张图片就帮助理解一下滑动窗口的机制：

![img](https://img-blog.csdn.net/20130801215525546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![img](https://img-blog.csdn.net/20130801215536296?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​    注意上图中的3个指针P1、P2、P3！此时接收窗口中接收的数据可能是失序的，但是也先存储在接收缓存之中。发送确认号的时候依然发送31，表示B期望接收的下一个数据报的标示符是31。

![img](https://img-blog.csdn.net/20130801215613015?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​    当B收到31后连同之前接收到的数据报，发送确认号34，此时A的滑动窗口可以向前移动了。

![img](https://img-blog.csdn.net/20130801215639062?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​    如果发送窗口中的数据报都属于已发送但未被确认的话，那么A就不能再继续发送数据，而需要进行等待。

![img](https://img-blog.csdn.net/20130801215708296?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2ljb2ZpZWxk/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

#### 2.TCP流量控制

​    所谓流量控制就是**让发送发送速率不要过快，让接收方来得及接收。利用滑动窗口机制就可以实施流量控制。**

​    原理这就是**运用TCP报文段中的窗口大小字段来控制，发送方的发送窗口不可以大于接收方发回的窗口大小。**

​    考虑一种特殊的情况，就是接收方若没有缓存足够使用，就会发送零窗口大小的报文，此时发送放将发送窗口设置为0，停止发送数据。之后接收方有足够的缓存，发送了非零窗口大小的报文，但是这个报文在中途丢失的，那么发送方的发送窗口就一直为零导致死锁。

​    解决这个问题，**TCP为每一个连接设置一个持续计时器**（persistence timer）。只要TCP的一方收到对方的零窗口通知，就启动该计时器，周期性的发送一个零窗口探测报文段。对方就在确认这个报文的时候给出现在的窗口大小（注意：TCP规定，即使设置为零窗口，也必须接收以下几种报文段：**零窗口探测报文段、确认报文段和携带紧急数据的报文段）**。

#### 3.传输效率及Nagle算法

​    TCP的数据传输分为交互数据流和成块数据流，交互数据流一般是一些交互式应用程序的命令，所以这些数据很小，而考虑到TCP报头和IP报头的总和就有40字节，如果数据量很小的话，那么网络的利用效率就较低。

​    数据传输使用Nagle算法，Nagle算法很简单，就是规定一个TCP连接最多只能有一个**未被确认的未完成的小分组**。在该分组的确认到达之前不能发送其他的小分组。

​    但是也要考虑另一个问题，叫做糊涂窗口综合症。当接收方的缓存已满的时候，交互应用程序一次只从缓存中读取一个字节（这时候缓存中腾出一个字节），然后向发送方发送确认信息，此时发送方再发送一个字节（收到的窗口大小为1），这样网络的效率很低。

​    所以要解决这个问题，可以让接收方等待一段时间，使得接收缓存已有最够的空间容纳一个最长报文段，或者等到接收缓存已有一半的空间。只要这两种情况出现一种，就发送确认报文，同时发送方可以把数据积累成大的报文段发送

### 3.3	TCP层与数据链路层流量控制的区别



  **数据链路层和TCP层都是面向连接的，都采用窗口协议来实现流量控制**，**然而两个窗口协议是不一样的。**

在**数据链路层，由于收发双方是点到点的连接**，其**流量控制策略相对较为简单**，接收窗口和发送窗口即为固定大小的缓冲区的个数，发送方的窗口调整，即缓冲区的覆盖依赖于确认帧的到达，由于信号传播延时和CPU的处理时间等都对相对较为稳定，所以发送方的数据帧和接收方的确认帧，其发送和接收时间是可估计的。

在TCP层，**由于一个TSAP可同时与多个TSAP建立连接，每个连接都将协商建立一个窗口**（即一对发送和接收缓冲区），所以窗口的管理较为复杂，其流量控制策略是通过窗口公告来实现的，当接收方收到数据后发送的确认中将通报剩余的接收缓冲区大小，发送方的发送窗口调整是根据接收方的窗口公告进行的，也就是即使收到接收方的确认也不一定就能对发送窗口进行调整，一旦发送方收到一个零窗口公告，必须暂停发送并等待接收方的下一个更新窗口公告，同时启动一个持续定时器。由于TCP层的收、发双方是端到端的，它面对的是一个网络，端到端的路径中可能包含多个点到点的链路，报文在整个传输过程中的延时难以估计甚至可能丢失，所以**在TCP的流量控制协议中规定：即使发送方收到了零窗口公告，在持续定时器超时后，允许发送一个字节的数据报文，要求接收方重申当前的窗口大小，以避免因接收方的更新窗口公告丢失而导致的死锁。**






## 4，TCP滑动窗口

TCP协议作为一个可靠的面向流的传输协议，其可靠性和流量控制由滑动窗口协议保证，而拥塞控制则由控制窗口结合一系列的控制算法实现。

#### 一、滑动窗口协议

   关于这部分自己不晓得怎么叙述才好，因为理解的部分更多，下面就用自己的理解来介绍下TCP的精髓：滑动窗口协议。
   所谓滑动窗口协议，自己理解有两点：

- “窗口”对应的是一段可以被发送者发送的字节序列，其连续的范围称之为“窗口”；

-  “滑动”则是指这段“允许发送的范围”是可以随着发送的过程而变化的，方式就是按顺序“滑动”。在引入一个例子来说这个协议之前，我觉得很有必要先了解以下**前提**：

  - TCP协议的两端分别为发送者A和接收者B，由于是全双工协议，因此A和B应该分别维护着一个独立的发送缓冲区和接收缓冲区，由于对等性（A发B收和B发A收），我们以A发送B接收的情况作为例子

  - 发送窗口是发送缓存中的一部分，是可以被TCP协议发送的那部分，其实应用层需要发送的所有数据都被放进了发送者的发送缓冲区；

  - 发送窗口中相关的有四个概念：**已发送并收到确认的数据**（不再发送窗口和发送缓冲区之内）、**已发送但未收到确认**的数据（位于发送窗口之中）、**允许发送但尚未发送**的数据以及发送窗口外发送缓冲区内**暂时不允许发送的数据**；

  - 每次成功发送数据之后，**发送窗口就会在发送缓冲区中按顺序移动**，将新的数据包含到窗口中准备发送；
    TCP建立连接的初始，B会告诉A自己的接收窗口大小，比如为‘20’：
    字节31-50为发送窗口
    ![img](http://blog.chinaunix.net/attachment/201402/17/26275986_1392626885IL2q.png)
    A发送11个字节后，发送窗口位置不变，B接收到了乱序的数据分组：
    ![img](http://blog.chinaunix.net/attachment/201402/17/26275986_1392627107R2FQ.png)
    只有当A成功发送了数据，即发送的数据得到了B的确认之后，才会移动滑动窗口离开已发送的数据；同时B则确认连续的数据分组，对于乱序的分组则先接收下来，避免网络重复传递：
    ![img](http://blog.chinaunix.net/attachment/201402/17/26275986_13926272726XTE.png)

    

#### **二、流量控制**

流量控制方面**主要有两个要点**需要掌握:

- TCP利用滑动窗口实现流量控制的机制
- 如何考虑流量控制中的传输效率。

##### **流量控制**

所谓流量控制，主要是接收方传递信息给发送方，使其不要发送数据太快，是一种端到端的控制。**主要的方式就是返回的ACK中会包含自己的接收窗口的大小，并且利用大小来控制发送方的数据发送：**
![img](http://blog.chinaunix.net/attachment/201402/17/26275986_1392627535jeG5.png)
这里面涉及到一种情况，**如果B已经告诉A自己的缓冲区已满，于是A停止发送数据**；等待一段时间后，B的缓冲区出现了富余，于是给A发送报文告诉A我的rwnd大小为400，但是这个报文不幸丢失了，于是就出现A等待B的通知||B等待A发送数据的死锁状态。为了处理这种问题，TCP引入了持续计时器（Persistence timer），当A收到对方的零窗口通知时，就启用该计时器，时间到则发送一个1字节的探测报文，对方会在此时回应自身的接收窗口大小，如果结果仍未0，则重设持续计时器，继续等待。

##### **传递效率**

一个显而易见的问题是：单个发送字节单个确认，和窗口有一个空余即通知发送方发送一个字节，无疑增加了网络中的许多不必要的报文（请想想为了一个字节数据而添加的40字节头部吧！），所以我们的原则是尽可能一次多发送几个字节，或者窗口空余较多的时候通知发送方一次发送多个字节。对于前者我们广泛使用Nagle算法，即：

- 若发送应用进程要把发送的数据逐个字节地送到TCP的发送缓存，则发送方就把第一个数据字节先发送出去，把后面的字节先缓存起来；
- 当发送方收到第一个字节的确认后（也得到了网络情况和对方的接收窗口大小），再把缓冲区的剩余字节组成合适大小的报文发送出去；
- 当到达的数据已达到发送窗口大小的一半或以达到报文段的最大长度时，就立即发送一个报文段；
  对于后者我们往往的做法是让接收方等待一段时间，或者接收方获得足够的空间容纳一个报文段或者等到接受缓存有一半空闲的时候，再通知发送方发送数据。

#### 三、拥塞控制

网络中的链路容量和交换结点中的缓存和处理机都有着工作的极限，当网络的需求超过它们的工作极限时，就出现了拥塞。拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。常用的方法就是：

- 慢开始、拥塞控制
- 快重传、快恢复

一切的基础还是慢开始，这种方法的思路是这样的：

- 发送方维持一个叫做“拥塞窗口”的变量，该变量和接收端口共同决定了发送者的发送窗口；

- 当主机开始发送数据时，避免一下子将大量字节注入到网络，造成或者增加拥塞，选择发送一个1字节的试探报文；

- 当收到第一个字节的数据的确认后，就发送2个字节的报文；

- 若再次收到2个字节的确认，则发送4个字节，依次递增2的指数级；

- 最后会达到一个提前预设的“慢开始门限”，比如24，即一次发送了24个分组，此时遵循下面的条件判定：

  - cwnd < ssthresh， 继续使用慢开始算法；
  - cwnd > ssthresh，停止使用慢开始算法，改用拥塞避免算法；
  - cwnd = ssthresh，既可以使用慢开始算法，也可以使用拥塞避免算法；
    

- 所谓拥塞避免算法就是：每经过一个往返时间RTT就把发送方的拥塞窗口+1，即让拥塞窗口缓慢地增大，按照线性规律增长；

- 当出现网络拥塞，比如丢包时，将慢开始门限设为原先的一半，然后将cwnd设为1，执行慢开始算法（较低的起点，指数级增长）；

  

  ![img](http://blog.chinaunix.net/attachment/201402/17/26275986_1392629245IG6b.png)

  上述方法的目的是在拥塞发生时循序减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够的时间把队列中积压的分组处理完毕。慢开始和拥塞控制算法常常作为一个整体使用，而快重传和快恢复则是为了减少因为拥塞导致的数据包丢失带来的重传时间，从而避免传递无用的数据到网络。快重传的机制是：

  - 接收方建立这样的机制，如果一个包丢失，则对后续的包继续发送针对该包的重传请求；
  - 一旦发送方接收到三个一样的确认，就知道该包之后出现了错误，立刻重传该包；
  - 此时发送方开始执行“快恢复”算法：
    - 慢开始门限减半；
    - cwnd设为慢开始门限减半后的数值；
    - 执行拥塞避免算法（高起点，线性增长）；
      ![img](http://blog.chinaunix.net/attachment/201402/17/26275986_1392629231ue0O.png)

## 5，TCP可靠传输


我们平常经常说UDP是不可靠连接，TCP是可靠连接，然而TCP为什么是可靠的呢



#### TCP和UDP的优缺点

##### TCP

- 缺点：
  - 三次握手四次挥手，传输更多包，浪费一些带宽
  - 为了进行可靠通信，双方都要维持在线，通信过程中服务器server可能出现非常大的并发连接，浪费了系统资源，甚至会出现宕机
  - 确认重传也会浪费一些带宽，且在不好的网络中，会不断的断开和连接，降低了传输效率
    

##### UDP

**优点：**

- 没有握手，起步快延时小
- 不需要维持双方在线，server不用维护巨量并发连接，节省了系统资源
- 没有重传机制，在不影响使用的情况下，能更高效的利用网络带宽

#### TCP相比UDP为什么是可靠的

##### [1] 确认和重传机制

建立连接时三次握手同步双方的“序列号 + 确认号 + 窗口大小信息”，是确认重传、流控的基础
传输过程中，如果Checksum校验失败、丢包或延时，发送端重传

##### [2] 数据排序

TCP有专门的序列号SN字段，可提供数据re-order

##### [3] 流量控制

窗口和计时器的使用。TCP窗口中会指明双方能够发送接收的最大数据量

##### [4] 拥塞控制

TCP的拥塞控制由4个核心算法组成。

“慢启动”（Slow Start）

“拥塞避免”（Congestion avoidance）

“快速重传 ”（Fast Retransmit）

“快速恢复”（Fast Recovery）

以上就是TCP比UDP传输更可靠的原因。



参考文章：
http://www.cnblogs.com/hupp/p/4856134.html
https://www.zhihu.com/question/49596182

## 6,TCP重传

其实，重传目前在网络的应用从编程的角度来看，用的比较少！就当作一种参考的工具吧！

TCP要保证所有的数据包都可以到达，所以，必需要有重传机制。 

**超时重传机制**：就是发送端死等接收端的ack，直到发送端超时之后，在发送一个包，直到收到接收端的ack为止。

例如：接收端给发送端的Ack确认只会确认最后一个连续的包，比如，发送端发了1,2,3,4,5一共五份数据，接收端收到了1，2，于是回ack 3，然后收到了4（注意此时3没收到），此时的TCP会怎么办？等待发送端的ACK 3，直到超时后，就会再发送3.面临一个艰难的选择，就是，是重传之前的一个还是重传所有的问题。对于上面的示例来说，是重传#3呢还是重传#3，#4，#5呢？



**快速重传机制**：这个机制不以时间为驱动，而是以数据来重传！如果接收端包收包没有连续到达，就ACK最后那个可能被丢了的包，如果发送方连续收到接收端3次相同的ack，就重传。

例如：如果发送方发出了1，2，3，4，5份数据，第一份先到送了，于是就ack回2，结果2因为某些原因没收到，3到达了，于是还是ack回2，后面的4和5都到了，但是还是ack回2，因为2还是没有收到，于是发送端收到了三个ack=2的确认，知道了2还没有到，于是就马上重转2。然后，接收端收到了2，此时因为3，4，5都收到了，于是ack回6。

它依然面临一个艰难的选择，就是，是重传之前的一个还是重传所有的问题。对于上面的示例来说，是重传#2呢还是重传#2，#3，#4，#5呢？因为发送端并不清楚这连续的3个ack(2)是谁传回来的？

于是进一步，引入了下面的机制：从发送端入手，选择确定(SACK)（参看[RFC 2018](http://tools.ietf.org/html/rfc2018)），这种方式需要在TCP头里加一个SACK的东西，ACK还是Fast Retransmit的ACK，SACK则是汇报收到的数据碎版。

![img](http://coolshell.cn//wp-content/uploads/2014/05/tcp_sack_example-1024x577.jpg)



这样，在发送端就可以根据回传的SACK来知道哪些数据到了，哪些没有到。于是就优化了Fast Retransmit的算法。当然，这个协议需要两边都支持。在 Linux下，可以通过tcp_sack参数打开这个功能（Linux 2.4后默认打开）。

这里还需要注意一个问题——接收方Reneging，所谓Reneging的意思就是接收方有权把已经报给发送端SACK里的数据给丢了。这样干是不被鼓励的，因为这个事会把问题复杂化了，但是，接收方这么做可能会有些极端情况，比如要把内存给别的更重要的东西。所以，发送方也不能完全依赖SACK，还是要依赖ACK，并维护Time-Out，如果后续的ACK没有增长，那么还是要把SACK的东西重传，另外，接收端这边永远不能把SACK的包标记为Ack。

注意：SACK会消费发送方的资源，试想，如果一个攻击者给数据发送方发一堆SACK的选项，这会导致发送方开始要重传甚至遍历已经发出的数据，这会消耗很多发送端的资源。详细的东西请参看《[TCP SACK的性能权衡](http://www.ibm.com/developerworks/cn/linux/l-tcp-sack/)》

##### Duplicate SACK – 重复收到数据的问题

Duplicate SACK又称D-SACK，其主要使用了SACK来告诉发送方有哪些数据被重复接收了。[RFC-2883 ](http://www.ietf.org/rfc/rfc2883.txt)里有详细描述和示例。下面举几个例子（来源于[RFC-2883](http://www.ietf.org/rfc/rfc2883.txt)）

D-SACK使用了SACK的第一个段来做标志，

- 如果SACK的第一个段的范围被ACK所覆盖，那么就是D-SACK

- 如果SACK的第一个段的范围被SACK的第二个段覆盖，那么就是D-SACK

**示例一：ACK丢包**

下面的示例中，丢了两个ACK，所以，发送端重传了第一个数据包（3000-3499），于是接收端发现重复收到，于是回了一个SACK=3000-3500，因为ACK都到了4000意味着收到了4000之前的所有数据，所以这个SACK就是D-SACK——旨在告诉发送端我收到了重复的数据，而且我们的发送端还知道，数据包没有丢，丢的是ACK包。

```
`Transmitted Received  ACK Sent``Segment   Segment   (Including SACK Blocks)` `3000-3499  3000-3499  3500 (ACK dropped)``3500-3999  3500-3999  4000 (ACK dropped)``3000-3499  3000-3499  4000, SACK=3000-3500``                  ``---------`
```

 **示例二，网络延误**

下面的示例中，网络包（1000-1499）被网络给延误了，导致发送方没有收到ACK，而后面到达的三个包触发了“Fast Retransmit算法”，所以重传，但重传时，被延误的包又到了，所以，回了一个SACK=1000-1500，因为ACK已到了3000，所以，这个SACK是D-SACK——标识收到了重复的包。

这个案例下，发送端知道之前因为“Fast Retransmit算法”触发的重传不是因为发出去的包丢了，也不是因为回应的ACK包丢了，而是因为网络延时了。

```
`Transmitted  Received  ACK Sent``Segment    Segment   (Including SACK Blocks)` `500-999    500-999   1000``1000-1499   (delayed)``1500-1999   1500-1999  1000, SACK=1500-2000``2000-2499   2000-2499  1000, SACK=1500-2500``2500-2999   2500-2999  1000, SACK=1500-3000``1000-1499   1000-1499  3000``        ``1000-1499  3000, SACK=1000-1500``                   ``---------`
```

 

可见，**引入了D-SACK，有这么几个好处**：

1）可以让发送方知道，是发出去的包丢了，还是回来的ACK包丢了。

2）是不是自己的timeout太小了，导致重传。

3）网络上出现了先发的包后到的情况（又称reordering）

4）网络上是不是把我的数据包给复制了。

 知道这些东西可以很好得帮助TCP了解网络情况，从而可以更好的做网络上的流控。

Linux下的tcp_dsack参数用于开启这个功能（Linux 2.4后默认打开）

最后，由于最近学习netstat命令，下面就通过一个服务器的命令结果，看下问题：

```html
TcpExt:
	79 invalid SYN cookies received
	8 resets received for embryonic SYN_RECV sockets
    9 packets pruned from receive queue because of socket buffer overrun
    5 ICMP packets dropped because they were out-of-window    132513 TCP sockets finished time wait in fast timer    5520362 delayed acks sent
    4854 delayed acks further delayed because of locked socket 
    Quick ack mode was activated 265388 times
    112119042 packets directly queued to recvmsg prequeue.    
    255626524 bytes directly in process context from backlog    2657617950 bytes directly received in process context from prequeue
    141 packets dropped from prequeue
    2779493398 packet headers predicted
    108242651 packets header predicted and directly queued to user
    209035695 acknowledgments not containing data payload received
    1531587862 predicted acknowledgments
    131551 times recovered from packet loss by selective acknowledgements
    279 bad SACK blocks received
    Detected reordering 10 times using FACK
    Detected reordering 19 times using SACK
    Detected reordering 37 times using time stamp
    214 congestion windows fully recovered without slow start 
    20 congestion windows partially recovered using Hoe heuristic
    3246 congestion windows recovered without slow start by DSACK
    61602 congestion windows recovered without slow start after partial ack
    TCPLostRetransmit: 31880
    12361 timeouts after SACK recovery
    289 timeouts in loss state
    498564 fast retransmits
    13967 forward retransmits
    42221 retransmits in slow start
    56690 other TCP timeouts
    TCPLossProbes: 1113019
    TCPLossProbeRecovery: 928744
    7366 SACK retransmits failed
    13 times receiver scheduled too late for direct processing 
    967 packets collapsed in receive queue due to low socket buffer
    267981 DSACKs sent for old packets
    520 DSACKs sent for out of order packets
    966679 DSACKs received
    1611 DSACKs for out of order packets received
    265 connections reset due to unexpected data
    105 connections reset due to early user close
    1132 connections aborted due to timeout
    TCPDSACKIgnoredOld: 4605
    TCPDSACKIgnoredNoUndo: 723291
    TCPSpuriousRTOs: 13632
    TCPSackShifted: 701131
    TCPSackMerged: 540339
    TCPSackShiftFallback: 1468281 
    TCPBacklogDrop: 7 
    TCPRetransFail: 126  
    TCPRcvCoalesce: 74902345  
    TCPOFOQueue: 61476920  
    TCPOFOMerge: 521 
    TCPChallengeACK: 1636  
    TCPSYNChallenge: 1600  
    TCPSpuriousRtxHostQueues: 159
```



```html
Detected reordering 10 times using FACK
Detected reordering 19 times using SACK
Detected reordering 37 times using time stamp
    
267981 DSACKs sent for old packets
520 DSACKs sent for out of order packets
966679 DSACKs received 
1611 DSACKs for out of order packets received
```

 从这里的统计数据可以看出，tcp对于这种数据的问题，采取上述的所有介绍的算法，

 time stamp，FACK，DSACK，SACK这些算法，了解网络失序有点帮忙，

 最终还是要抓包来看问题！

 至于其他的内容，需要等网络学习完毕之后，

 再来一一分析！

## 7，TCP拆包/黏包机器解决方法

- TCP粘包/拆包
  - TCP粘包/拆包问题说明
  - TCP粘包/拆包发生的原因
  - 粘包问题的解决策略
- 未考虑TCP粘包导致功能异常案例 
  - TimeServer的改造
  - TimeClient的改造
- 利用LineBasedFrameDecoder解决TCP粘包问题
  - LineBasedFrameDecoder和StringDecoder的原理分析

 

------

无论是服务端还是客户端，当我们读取或者发送消息的时候，都需要考虑TCP底层的粘包/拆包机制。

#### TCP粘包/拆包

TCP是个“流”协议，所谓流，就是没有界限的一串数据。大家可以想想河里的流水，是连成一片的，其间并没有分界线。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的划分，所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。



##### TCP粘包/拆包问题说明

![img](https://images2015.cnblogs.com/blog/990532/201612/990532-20161212192914995-1758321651.png)

假设客户端分别发送了两个数据包D1和D2给服务端，由于服务端一次读取到的字节数是不确定的，故可能存在以下4种情况。

（1）服务端分两次读取到了两个独立的数据包，分别是D1和D2，没有粘包和拆包；

（2）服务端一次接收到了两个数据包，D1和D2粘合在一起，被称为TCP粘包；

（3）服务端分两次读取到了两个数据包，第一次读取到了完整的D1包和D2包的部分内容，第二次读取到了D2包的剩余内容，这被称为TCP拆包；

（4）服务端分两次读取到了两个数据包，第一次读取到了D1包的部分内容D1_1，第二次读取到了D1包的剩余内容D1_2和D2包的整包。

如果此时服务端TCP接收滑窗非常小，而数据包D1和D2比较大，很有可能会发生第五种可能，即服务端分多次才能将D1和D2包接收完全，期间发生多次拆包。



##### TCP粘包/拆包发生的原因

问题产生的原因有三个，分别如下。

（1）应用程序write写入的字节大小大于套接口发送缓冲区大小；

（2）进行MSS大小的TCP分段；

（3）以太网帧的payload大于MTU进行IP分片。

 ![img](https://images2015.cnblogs.com/blog/990532/201612/990532-20161212193751011-992309759.png)



##### 粘包问题的解决策略

由于底层的TCP无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈设计来解决，根据业界的主流协议的解决方案，可以归纳如下。

（1）消息定长，例如每个报文的大小为固定长度200字节，如果不够，空位补空格；

（2）在包尾增加回车换行符进行分割，例如FTP协议；

（3）将消息分为消息头和消息体，消息头中包含表示消息总长度（或者消息体长度）的字段，通常设计思路为消息头的第一个字段使用int32来表示消息的总长度；

（4）更复杂的应用层协议。



##### 未考虑TCP粘包导致功能异常案例 

在前面的时间服务器例程中，我们多次强调并没有考虑读半包问题，这在功能测试时往往没有问题，但是一旦压力上来，或者发送大报文之后，就会存在粘包/拆包问题。如果代码没有考虑，往往就会出现解码错位或者错误，导致程序不能正常工作。以[Netty 入门示例](http://www.cnblogs.com/wade-luffy/p/6165626.html)为例。



###### TimeServer的改造



```
import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerAdapter;
import io.netty.channel.ChannelHandlerContext;


public class TimeServerHandler extends ChannelHandlerAdapter {

    private int counter;

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg)
            throws Exception {
        ByteBuf buf = (ByteBuf) msg;
        byte[] req = new byte[buf.readableBytes()];
        buf.readBytes(req);
        String body = new String(req, "UTF-8").substring(0, req.length - System.getProperty("line.separator").length());
        System.out.println("The time server receive order : " + body + " ; the counter is : " + ++counter);
        String currentTime = "QUERY TIME ORDER".equalsIgnoreCase(body) ?
                new java.util.Date( System.currentTimeMillis()).toString() : "BAD ORDER";
        currentTime = currentTime + System.getProperty("line.separator");
        ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes());
        ctx.writeAndFlush(resp);
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        ctx.close();
    }
}
```



每读到一条消息后，就计一次数，然后发送应答消息给客户端。按照设计，服务端接收到的消息总数应该跟客户端发送的消息总数相同，而且请求消息删除回车换行符后应该为"QUERY TIME ORDER"。





```
import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerAdapter;
import io.netty.channel.ChannelHandlerContext;

public class TimeClientHandler extends ChannelHandlerAdapter {

    private int counter;

    private byte[] req;

    public TimeClientHandler() {
        req = ("QUERY TIME ORDER" + System.getProperty("line.separator")).getBytes();
    }

    @Override
    public void channelActive(ChannelHandlerContext ctx) {
        ByteBuf message = null;
        for (int i = 0; i < 100; i++) {
            message = Unpooled.buffer(req.length);
            message.writeBytes(req);
            ctx.writeAndFlush(message);
        }
    }

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg)
            throws Exception {
        ByteBuf buf = (ByteBuf) msg;
        byte[] req = new byte[buf.readableBytes()];
        buf.readBytes(req);
        String body = new String(req, "UTF-8");
        System.out.println("Now is : " + body + " ; the counter is : " + ++counter);
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        // 释放资源
        ctx.close();
    }
}
```



客户端跟服务端链路建立成功之后，循环发送100条消息，每发送一条就刷新一次，保证每条消息都会被写入Channel中。按照我们的设计，服务端应该接收到100条查询时间指令的请求消息。客户端每接收到服务端一条应答消息之后，就打印一次计数器。按照设计初衷，客户端应该打印100次服务端的系统时间。

**运行结果：**

服务端运行结果如下。

The time server receive order : QUERY TIME ORDER

QUERY TIME ORDER

......................

QUERY TIME ORDER ; the counter is : 1

The time server receive order :

QUERY TIME ORDER

............

QUERY TIME ORDER ; the counter is : 2

服务端运行结果表明它只接收到了两条消息，第一条包含57条“QUERY TIME ORDER”指令，第二条包含了43条“QUERY TIME ORDER”指令，总数正好是100条。我们期待的是收到100条消息，每条包含一条“QUERY TIME ORDER”指令。这说明发生了TCP粘包。

客户端运行结果如下。

Now is : BAD ORDER

BAD ORDER

; the counter is : 1

按照设计初衷，客户端应该收到100条当前系统时间的消息，但实际上只收到了一条。这不难理解，因为服务端只收到了2条请求消息，所以实际服务端只发送了2条应答，由于请求消息不满足查询条件，所以返回了2条“BAD ORDER”应答消息。但是实际上客户端只收到了一条包含2条“BAD ORDER”指令的消息，说明服务端返回的应答消息也发生了粘包。由于上面的例程没有考虑TCP的粘包/拆包，所以当发生TCP粘包时，我们的程序就不能正常工作。



###### 利用LineBasedFrameDecoder解决TCP粘包问题

为了解决TCP粘包/拆包导致的半包读写问题，Netty默认提供了多种编解码器用于处理半包，只要能熟练掌握这些类库的使用，TCP粘包问题从此会变得非常容易，你甚至不需要关心它们，这也是其他NIO框架和JDK原生的NIO API所无法匹敌的。

服务端代码：

```
import io.netty.bootstrap.ServerBootstrap;
import io.netty.channel.*;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.nio.NioServerSocketChannel;
import io.netty.handler.codec.LineBasedFrameDecoder;
import io.netty.handler.codec.string.StringDecoder;

public class TimeServer {

    public void bind(int port) throws Exception {
　　　　 // 配置服务端的NIO线程组
        EventLoopGroup bossGroup = new NioEventLoopGroup();
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        try {
            ServerBootstrap b = new ServerBootstrap();
            b.group(bossGroup, workerGroup)
                    .channel(NioServerSocketChannel.class)
                    .option(ChannelOption.SO_BACKLOG, 1024)
                    .childHandler(new ChildChannelHandler());
            // 绑定端口，同步等待成功
            ChannelFuture f = b.bind(port).sync();

            // 等待服务端监听端口关闭
            f.channel().closeFuture().sync();
        } finally {
            // 优雅退出，释放线程池资源
            bossGroup.shutdownGracefully();
            workerGroup.shutdownGracefully();
        }
    }

    private class ChildChannelHandler extends ChannelInitializer {
        @Override
        protected void initChannel(Channel arg0) throws Exception {
            arg0.pipeline().addLast(new LineBasedFrameDecoder(1024));
            arg0.pipeline().addLast(new StringDecoder());
            arg0.pipeline().addLast(new TimeServerHandler());
        }
    }

    public static void main(String[] args) throws Exception {
        int port = 8080;
        if (args != null && args.length > 0) {
            try {
                port = Integer.valueOf(args[0]);
            } catch (NumberFormatException e) {
                // 采用默认值
            }
        }
        new TimeServer().bind(port);
    }
}

import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerAdapter;
import io.netty.channel.ChannelHandlerContext;


public class TimeServerHandler extends ChannelHandlerAdapter {

    private int counter;

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg)
            throws Exception {
        String body = (String) msg;
        System.out.println("The time server receive order : " + body  + " ; the counter is : " + ++counter);
        String currentTime = "QUERY TIME ORDER".equalsIgnoreCase(body) ?
                new java.util.Date(System.currentTimeMillis()).toString() : "BAD ORDER";
        currentTime = currentTime + System.getProperty("line.separator");
        ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes());
        ctx.writeAndFlush(resp);
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        ctx.close();
    }
}
```



客户端代码：



```
import io.netty.bootstrap.Bootstrap;
import io.netty.channel.*;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.nio.NioSocketChannel;
import io.netty.handler.codec.LineBasedFrameDecoder;
import io.netty.handler.codec.string.StringDecoder;


public class TimeClient {

    public void connect(int port, String host) throws Exception {
// 配置客户端NIO线程组
        EventLoopGroup group = new NioEventLoopGroup();
        try {
            Bootstrap b = new Bootstrap();
            b.group(group).channel(NioSocketChannel.class)
                    .option(ChannelOption.TCP_NODELAY, true)
                    .handler(new ChannelInitializer() {
                        @Override
                        public void initChannel(Channel ch)
                                throws Exception {
                            ch.pipeline().addLast(new LineBasedFrameDecoder(1024));
                            ch.pipeline().addLast(new StringDecoder());
                            ch.pipeline().addLast(new TimeClientHandler());
                        }
                    });

            // 发起异步连接操作
            ChannelFuture f = b.connect(host, port).sync();

            // 等待客户端链路关闭
            f.channel().closeFuture().sync();
        } finally {
            // 优雅退出，释放NIO线程组
            group.shutdownGracefully();
        }
    }

    public static void main(String[] args) throws Exception {
        int port = 8080;
        if (args != null && args.length > 0) {
            try {
                port = Integer.valueOf(args[0]);
            } catch (NumberFormatException e) {
                // 采用默认值
            }
        }
        new TimeClient().connect(port, "127.0.0.1");
    }
}


import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerAdapter;
import io.netty.channel.ChannelHandlerContext;


public class TimeClientHandler extends ChannelHandlerAdapter {

    private int counter;

    private byte[] req;

    public TimeClientHandler() {
        req = ("QUERY TIME ORDER" + System.getProperty("line.separator"))
                .getBytes();
    }

    @Override
    public void channelActive(ChannelHandlerContext ctx) {
        ByteBuf message = null;
        for (int i = 0; i < 100; i++) {
            message = Unpooled.buffer(req.length);
            message.writeBytes(req);
            ctx.writeAndFlush(message);
        }
    }

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg)
            throws Exception {
        String body = (String) msg;
        System.out.println("Now is : " + body + " ; the counter is : "  + ++counter);
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {
        // 释放资源
        ctx.close();
    }
}
```



两个变化：

1. 拿到的msg已经是解码成字符串之后的应答消息
2. 新增了两个解码器：第一个是LineBasedFrameDecoder，第二个是StringDecoder。

运行结果：

服务端执行结果如下。

The time server receive order : QUERY TIME ORDER ; the counter is : 1

.....................................

The time server receive order : QUERY TIME ORDER ; the counter is : 100

客户端运行结果如下。

Now is : Thu Feb 20 00:00:14 CST 2014 ; the counter is : 1

......................................

Now is : Thu Feb 20 00:00:14 CST 2014 ; the counter is : 100

程序的运行结果完全符合预期，说明通过使用LineBasedFrameDecoder和StringDecoder成功解决了TCP粘包导致的读半包问题。对于使用者来说，只要将支持半包解码的handler添加到ChannelPipeline中即可，不需要写额外的代码，用户使用起来非常简单。



###### LineBasedFrameDecoder和StringDecoder的原理分析

LineBasedFrameDecoder的工作原理是它依次遍历ByteBuf中的可读字节，判断看是否有“\n”或者“\r\n”，如果有，就以此位置为结束位置，从可读索引到结束位置区间的字节就组成了一行。它是以换行符为结束标志的解码器，支持携带结束符或者不携带结束符两种解码方式，同时支持配置单行的最大长度。如果连续读取到最大长度后仍然没有发现换行符，就会抛出异常，同时忽略掉之前读到的异常码流。

StringDecoder的功能非常简单，就是将接收到的对象转换成字符串，然后继续调用后面的handler。LineBasedFrameDecoder + StringDecoder组合就是按行切换的文本解码器，它被设计用来支持TCP的粘包和拆包。

如果发送的消息不是以换行符结束的该怎么办呢？或者没有回车换行符，靠消息头中的长度字段来分包怎么办？是不是需要自己写半包解码器？答案是否定的，Netty提供了多种支持TCP粘包/拆包的解码器，用来满足用户的不同诉求。

 



## 8，TCP详解

#### 数据传输

　　在TCP的数据传送状态，很多重要的机制保证了TCP的可靠性和强壮性。它们包括：使用序号，对收到的TCP报文段进行排序以及检测重复的数据；使用校验和来检测报文段的错误；使用确认和计时器来检测和纠正丢包或延时。
　　在TCP的连接创建状态，两个主机的TCP层间要交换初始序号（ISN:initial sequence number）。这些序号用于标识字节流中的数据，并且还是对应用层的数据字节进行记数的整数。通常在每个TCP报文段中都有一对序号和确认号。TCP报文发送者认为自己的字节编号为序号，而认为接收者的字节编号为确认号。TCP报文的接收者为了确保可靠性，在接收到一定数量的连续字节流后才发送确认。这是对TCP的一种扩展，通常称为选择确认（Selective Acknowledgement）。选择确认使得TCP接收者可以对乱序到达的数据块进行确认。每一个字节传输过后，ISN号都会递增1。
　　通过使用序号和确认号，TCP层可以把收到的报文段中的字节按正确的顺序交付给应用层。序号是32位的无符号数，在它增大到2^32-1时，便会回绕到0。对于ISN的选择是TCP中关键的一个操作，它可以确保强壮性和安全性。
来看个例子：
　　1）发送方首先发送第一个包含序列号为1（可变化）和1460字节数据的TCP报文段给接收方。接收方以一个没有数据的TCP报文段来回复（只含报头），用确认号1461来表示已完全收到并请求下一个报文段。
　　2）发送方然后发送第二个包含序列号为1461和1460字节数据的TCP报文段给接收方。正常情况下，接收方以一个没有数据的TCP报文段来回复，用确认号2921（1461+1460）来表示已完全收到并请求下一个报文段。发送接收这样继续下去。
　　3）然而当这些数据包都是相连的情况下，接收方没有必要每一次都回应。比如，他收到第1到5条TCP报文段，只需回应第五条就行了。在例子中第3条TCP报文段被丢失了，所以尽管他收到了第4和5条，然而他只能回应第2条。
　　4）发送方在发送了第三条以后，没能收到回应，因此当时钟（timer）过时（expire）时，他重发第三条。（每次发送者发送一条TCP报文段后，都会再次启动一次时钟：RTT）。
　　5）这次第三条被成功接收，接收方可以直接确认第5条，因为4，5两条已收到。
　　
　　 ![img](https://img-blog.csdn.net/20150718102515107) 

　　Acknowledgment Number Out = Sequence Number In + Bytes of Data Received

#### 1.TCP重传

　　报文重传是TCP最基本的错误恢复功能，它的目的是防止报文丢失。
　　报文丢失的可能因素有很多种，包括应用故障，路由设备过载，或暂时的服务宕机。报文级别速度是很高的，而通常报文丢失是暂时的，因此TCP能够发现和恢复报文丢失显得尤为重要。

　　重传机制在实现数据可靠传输功能的同时，也引起了相应的性能问题：何时进行数据重传？如何保证较高的传输效率？
　　重传时间过短：在网络因为拥塞引起丢包时，频繁的重传会进一步加剧网络拥塞，引起丢包，恶化网络传输性能。
　　重传时间过长：接收方长时间无法完成数据接收，引起长时间占用连接线路造成资源损耗、传输效率较低等问题。
　　针对上述问题，TCP中设计了超时重传机制。该机制规定当发送方A向B发送数据包P1时，开启时长为RTO（Retransmission Timeout）的重传定时器，如果A在RTO内未收到B对P1的确认报文，则认为P1在网络中丢失，此时重新发送P1。由此，引出RTO大小的设定问题。

　　决定报文是否有必要重传的主要机制是重传计时器（retransmission timer），它的主要功能是维护重传超时（RTO）值。当报文使用TCP传输时，重传计时器启动，收到ACK时计时器停止。报文发送至接收到ACK的时间称为往返时间（RTT）。对若干次时间取平均值，该值用于确定最终RTO值。在最终RTO值确定之前，确定每一次报文传输是否有丢包发生使用重传计时器，下图说明了TCP重传过程。
　　

 ![img](https://img-blog.csdn.net/20150718102804067) 

　　当报文发送之后，但接收方尚未发送TCP ACK报文，发送方假设源报文丢失并将其重传。重传之后，RTO值加倍；如果在2倍RTO值到达之前还是没有收到ACK报文，就再次重传。如果仍然没有收到ACK，那么RTO值再次加倍。如此持续下去，每次重传RTO都翻倍，直到收到ACK报文或发送方达到配置的最大重传次数。
　　最大重传次数取决于发送操作系统的配置值。默认情况下，Windows主机默认重传5次。大多数Linux系统默认最大15次。两种操作系统都可配置。

##### 1）超时重传

　　超时重传机制用来保证TCP传输的可靠性。每次发送数据包时，发送的数据报都有seq号，接收端收到数据后，会回复ack进行确认，表示某一seq号数据已经收到。发送方在发送了某个seq包后，等待一段时间，如果没有收到对应的ack回复，就会认为报文丢失，会重传这个数据包。

##### 2）快速重传

　　接受数据一方发现有数据包丢掉了（并不是所期望的值。这意味着报文在传送中丢失。接收端注意到报文乱序，并且在第三个报文中发送重复ACK）。就会发送重复ACK报文告诉发送端重传丢失的报文。
　　当重传主机从发送端接收到3个重复ACK时，它会假设此报文确实在传送中丢失，并且立即发送一个快速重传。一旦触发了快速重传，所有正在传输的其他报文都被放入队列中，直到快速重传报文发送为止。过程如下图所示：

　　 ![img](https://img-blog.csdn.net/20150718102858936) 

　　比较超时重传和快速重传，可以发现超时重传是发送端在傻等超时，然后触发重传；而快速重传则是接收端主动告诉发送端数据没收到，然后触发发送端重传。
　　由此可看出，快速重传机制在一定程度上弥补了超时重传机制，使得重传更加及时。

#### 2.流量控制

　　这里主要说TCP滑动窗口流量控制。滑动窗口(Sliding window )是一种流量控制技术。早期的网络通信中,通信双方不会考虑网络的拥挤情况直接发送数据。由于大家不知道网络拥塞状况，一起发送数据,导致中间结点阻塞掉包,谁也发不了数据。所以就有了滑动窗口机制来解决此问题。
　　为了理解TCP的窗口大小是怎么样变化的，我们先需要理解它的含义。最简单的方式就是认为窗口大小”意味着接收方能接收数据的大小”，这也是说接收端设备再应用程序读取buffer中数据之前，能从对端连接处理多少数据。比如说server端窗口大小是360，那么就意味着server端一次只能从客户端接收不超过360bytes的数据。当server端收到数据，它会将数据放到buffer里，然后server端必须对这份数据做两件事：
　　1）server端必须发送一个 ACK 到client端来确认数据已经收到
　　2）server端必须处理这份数据，把它交给对应的应用程序
　　要区分上面两件事对理解窗口很重要，接收方收到数据后会确认，但是数据并不一定是里面就是从buffer里取出的，这是受应用层逻辑控制的。所以很有可能如果接收数据过快，而取出数据更慢，就会导致buffer满。一旦这种情况发生，窗口大小就开始调整来防止接收方负载过高。

　　TCP头里有一个字段叫Window，又叫Advertised-Window，这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。　　　Window是一个16bit位字段，它代表的是窗口的字节容量，也就是TCP的标准窗口最大为2^16-1=65535个字节。
　　另外在TCP的选项字段中还包含了一个TCP窗口扩大因子，option-kind为3，option-length为3个字节，option-data取值范围0-14。窗口扩大因子用来扩大TCP窗口，可把原来16bit的窗口，扩大为31bit。
　　
　　 ![img](https://img-blog.csdn.net/20150718103040083) 

　　1）对于TCP会话的发送方，任何时候在其发送缓存内的数据都可以分为4类，“已经发送并得到对端ACK的”，“已经发送但还未收到对端ACK的”，“未发送但对端允许发送的”，“未发送且对端不允许发送”。“已经发送但还未收到对端ACK的”和“未发送但对端允许发送的”这两部分数据称之为发送窗口。

　　 ![img](https://img-blog.csdn.net/20150718103120164)  

　　当收到接收方新的ACK对于发送窗口中后续字节的确认是，窗口滑动，滑动原理如下图：

　　 ![img](https://img-blog.csdn.net/20150718103157063) 

　　一个例子：

　　 ![img](https://img-blog.csdn.net/20150718103218167) 

#### 滑动窗口协议

##### 1）比特滑动窗口协议

　　当发送窗口和接收窗口的大小固定为1时，滑动窗口协议退化为停等协议（stop－and－wait）。该协议规定发送方每发送一帧后就要停下来，等待接收方已正确接收的确认（acknowledgement）返回后才能继续发送下一帧。由于接收方需要判断接收到的帧是新发的帧还是重新发送的帧，因此发送方要为每一个帧加一个序号。由于停等协议规定只有一帧完全发送成功后才能发送新的帧，因而只用一比特来编号就够了。

##### 2）后退n协议

　　由于停等协议要为每一个帧进行确认后才继续发送下一帧，大大降低了信道利用率，因此又提出了后退n协议。后退n协议中，发送方在发完一个数据帧后，不停下来等待应答帧，而是连续发送若干个数据帧，即使在连续发送过程中收到了接收方发来的应答帧，也可以继续发送。且发送方在每发送完一个数据帧时都要设置超时定时器。只要在所设置的超时时间内仍未收到确认帧，就要重发相应的数据帧。如：当发送方发送了N个帧后，若发现该N帧的前一个帧在计时器超时后仍未返回其确认信息，则该帧被判为出错或丢失，此时发送方就不得不重新发送出错帧及其后的N帧。

来看下面的例子，这里假设n=9：

　　 ![img](https://img-blog.csdn.net/20150718103400908) 

　　首先发送方一口气发送10个数据帧，前面两个帧正确返回了，数据帧2出现了错误，这时发送方被迫重新发送2-8这7个帧，接受方也必须丢弃之前接受的3-8这几个帧。

　　从这里不难看出，后退n协议一方面因连续发送数据帧而提高了效率，但另一方面，在重传时又必须把原来已正确传送过的数据帧进行重传（仅因这些数据帧之前有一个数据帧出了错），这种做法又使传送效率降低。由此可见，若传输信道的传输质量很差因而误码率较大时，连续测协议不一定优于停止等待协议。此协议中的发送窗口的大小为k，接收窗口仍是1。

##### 3）选择重传协议

　　在后退n协议中，接收方若发现错误帧就不再接收后续的帧，即使是正确到达的帧，这显然是一种浪费。另一种效率更高的策略是当接收方发现某帧出错后，其后继续送来的正确的帧虽然不能立即递交给接收方的高层，但接收方仍可收下来，存放在一个缓冲区中，同时要求发送方重新传送出错的那一帧。
　　但是必须强调一点,接收方永远不会把分组失序地交给应用层.在他们被交付给应用层之前,先要等待那些更早发出来的分组到达。一旦收到重新传来的帧后，就可以原已存于缓冲区中的其余帧一并按正确的顺序递交高层。这种方法称为选择重发(SELECTICE REPEAT)，其工作过程如图所示。显然，选择重发减少了浪费，但要求接收方有足够大的缓冲区空间。
　　
　　 ![img](https://img-blog.csdn.net/20150718103441574) 

##### 4）零窗口问题

　　某些情况下，服务器无法再处理从客户端发送的数据。可能是由于内存不足，处理能力不够，或其他原因。这可能会造成数据被丢弃以及传输暂停，但接收窗口能够帮助减小负面影响。
　　当上述情况发生时，服务器会发送窗口为0的报文。当客户端接收到此报文时，它会暂停所有数据传输，但会保持与服务器的连接以传输探测（keep-alive Zero Window Probe）报文。探测报文在客户端以稳定间隙发送，以查看服务器接收窗口状态。一旦服务器能够再次处理数据，将会返回非零值窗口大小，传输会恢复。下图示例了零窗口通知过程。

　　 ![img](https://img-blog.csdn.net/20150718103512042) 

##### 5）Silly Window Syndrome（糊涂窗口综合症）

　　Silly window syndrome定义为一次仅发送少量的TCP负载数据，就像用一个飞机只运送你一个人（你又不是总统，哼），这种情况下带宽利用率很低，一般尽量避免。

　　 ![img](https://img-blog.csdn.net/20150718103623727) 

　　对接收端来说，window size小于某个值，可以直接ack(0)回sender，这样就把window给关闭了，也阻止了sender再发数据过来。当接收端size重新达到MSS或者接收端缓存区的一半.
　　对于发送端来说，呵呵，不是有Nagle’s algorithm嘛。这个算法的思路也是延时处理，两个主要的条件1）要等到 Window Size>=MSS 或是 Data Size >=MSS，2）等待时间或是超时200ms，满足这两个条件之一再发送，否则就是就接着攒数据。

#### 3.拥塞控制

　　滑动窗用来做流量控制。流量控制只关注发送端和接受端自身的状况，而没有考虑整个网络的通信情况。拥塞控制，则是基于整个网络来考虑的。考虑一下这样的场景：某一时刻网络上的延时突然增加，那么，TCP对这个事做出的应对只有重传数据，但是，重传会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，于是，这个情况就会进入恶性循环被不断地放大。试想一下，如果一个网络内有成千上万的TCP连接都这么行事，那么马上就会形成“网络风暴”，TCP这个协议就会拖垮整个网络。为此，TCP引入了拥塞控制策略。拥塞策略算法主要包括：**慢启动，拥塞避免，拥塞发生，快速恢复**。

　　对于拥塞现象，我们可以进一步用图1来描述。当网络负载较小时，吞吐量基本上随着负载的增长而增长，呈线性关系，响应时间增长缓慢。当负载达到网络容量时，吞吐量呈现出缓慢增长，而响应时间急剧增加，这一点称为Knee。假如负载继续增加，路由器开始丢包，当负载超过一定量时，吞吐量开始急剧下降，这一点称为Cliff。
　　拥塞控制机制实际上包含**拥塞避免（congestion avoidance）和拥塞控制（congestion control）**两种策略。前者的目的是使网络运行在Knee四周，**避免拥塞的发生**；而后者则是使得网络运行在Cliff的左侧区域。**前者是一种“预防”措施，维持网络的高吞吐量、低延迟状态，避免进入拥塞；后者是一种“恢复”措施，使网络从拥塞中恢复过来，进入正常的运行状态**。



 ![img](https://img-blog.csdn.net/20150718103749387) 　　　　　　　　　

　　拥塞发生的主要原因在于**网络能够提供的资源不足以满足用户的需求**，这些资源包括缓存空间、链路带宽容量和中间节点的处理能力。由于互联网的设计机制导致其缺乏“接纳控制”能力，因此在网络资源不足时不能限制用户数量，而只能靠降低服务质量来继续为用户服务，也就是“尽力而为”的服务。
　　**拥塞虽然是由于网络资源的稀缺引起的，但单纯增加资源并不能避免拥塞的发生**。例如增加缓存空间到一定程度时，只会加重拥塞，而不是减轻拥塞，这是因为当数据包经过长时间排队完成转发时，它们很可能早已超时，从而引起源端超时重发，而这些数据包还会继续传输到下一路由器，从而浪费网络资源，加重网络拥塞。同样如果单纯增加链路带宽，如果中间路由来不及好处理，就会造成大量丢包，引发拥塞。
　　单纯地增加网络资源之所以不能解决拥塞问题，是因为拥塞本身是一个动态问题，它不可能只靠静态的方案来解决，而需要协议能够在网络出现拥塞时保护网络的正常运行。目前对互联网进行的拥塞控制主要是依靠在源端执行的基于窗口的TCP拥塞控制机制。网络本身对拥塞控制所起的作用较小。

　　拥塞控制假设分组的丢失都是由网络繁忙造成的。拥塞控制有三种动作，分别对应主机感受到的情况：
　　收到一条新确认。这很好，表明当前的单次发送量小于网络的承载量。
　　收到三条对同一分组的确认，即三条重复的确认。单次发送量往往大于3，例如发送序号为0、10、20、30、40的5条长度为10字节的分组，其中序号20的丢了，则返回的确认是10、20、20、20。3个20就是重复的确认。
　　对某一条分组的确认迟迟未到，即超时。例如发送序号为0、10、20、30、40的5条长度为10字节的分组，其中序号30的丢了，则返回的确认是10、20、30、30。这才只有两条重复确认。然而刚刚说过，单次发送量往往大于3，所以超时更可能是因为不止一条分组或确认丢失而引起的，这说明网络比上一情况中的更加繁忙。

　　**上面提到拥塞控制主要是四个算法：1）慢启动，2）拥塞避免，3）拥塞发生，4）快速恢复。**
　　1988年，TCP-Tahoe 提出了1）慢启动，2）拥塞避免，3）拥塞发生时的快速重传
　　1990年，TCP Reno 在Tahoe的基础上增加了4）快速恢复

　　 ![img](https://img-blog.csdn.net/20150718103857667)

 ![img](https://img-blog.csdn.net/20150718103912626)  

##### １)慢开始算法:指数增加

![image-20200211183725922](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211183725922.png)

　![image-20200211183807983](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211183807983.png)![image-20200211183827916](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211183827916.png)

![image-20200211190702003](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211190702003.png)

![image-20200211190724062](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211190724062.png)

##### ２）拥塞避免：加法增加

　　**以慢速启动算法开始，则拥塞窗口大小按指数规则增长，这样增长太快了。为了在拥塞发生之前避免拥塞，必须降低指数增长的速度。**
　　**拥塞避免的主要思想是加法增大，也就是cwnd的值不再指数级往上升**，开始加法增加。此时当窗口中所有的报文段都被确认时，cwnd的大小加1，cwnd的值就随着RTT开始线性增加，这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。

![image-20200211190827727](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211190827727.png)

![image-20200211190858411](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211190858411.png)

![image-20200211190938252](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211190938252.png)

##### ３）快重传算法：乘性减少

![image-20200211191013791](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211191013791.png)

　　上面讨论的两个机制都是没有检测到拥塞的情况下的行为，那么当发现拥塞了cwnd又该怎样去调整呢？

　　 ![img](https://img-blog.csdn.net/20150718104228469) 

　　之前提到过，重传是在两种情况下发生：
　　1）如果RTO超时，那么存在非常严重的拥塞的可能性；包可能已在网络中丢失。
　　在这种情况下，TCP做出强烈的反应。
　　a.设置阈值为cwnd的一半。
　　b.重新设置cwnd为1。SSS
　　c.启动慢速启动阶段。

　　2）如果收到3个相同的ACK，那么存在着轻度拥塞的可能性。TCP在收到乱序到达包时就会立即发送ACK，TCP利用3个相同的ACK来判定数据包的丢失，此时进行快速重传。
在这种情况下，TCP做出轻度的反应。
　　a.设置阈值为cwnd的一半。
　　b.设置cwnd为阈值（有些实现是阈值加上3）
　　c.启动拥塞避免阶段。



##### ４）快速恢复

![image-20200211191049960](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211191049960.png)

![image-20200211191117177](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211191117177.png)

　　“快速恢复”算法是在“快速重传”算法后添加的，当收到3个重复ACK时，TCP最后进入的不是拥塞避免阶段，而是快速恢复阶段。**快速重传和快速恢复算法一般同时使用**。快速恢复的思想是“数据包守恒”原则，即同一个时刻在网络中的数据包数量是恒定的，只有当“老”数据包离开了网络后，才能向网络中发送一个“新”的数据包，如果发送方收到一个重复的ACK，那么根据TCP的ACK机制就表明有一个数据包离开了网络，于是cwnd加1。如果能够严格按照该原则那么网络中很少会发生拥塞，事实上拥塞控制的目的也就在修正违反该原则的地方。
　　**快速恢复状态是一种介于慢启动和拥塞避免之间的状态**。它像慢启动，其中cwnd以指数增长，但是cwnd以ssthresh加3MSS（而不是1）开始。当TCP进入快速恢复阶段，可能发生三种主要事件。如果重复ACK继续到达，那么TCP保持这种状态，但是cwnd呈指数增长。如果发生超时，TCP假设网络中有真实的拥塞，并进入慢启动状态。如果一个新的（非重复）ACK到达，TCP进入拥塞避免阶段，但是将cwnd的大小减小到ssthresh值，好像三次重复ACK没有发生过一样，并且转换是从慢启动状态到拥塞避免状态。
　　具体来说快速恢复的主要步骤是：
　　1.当收到3个重复ACK时，把ssthresh设置为cwnd的一半，把cwnd设置为ssthresh的值加3，然后重传丢失的报文段，加3的原因是因为收到3个重复的ACK，表明有3个“老”的数据包离开了网络。
　　2.再收到重复的ACK时，拥塞窗口增加1。
　　3.当收到新的数据包的ACK时，把cwnd设置为第一步中的ssthresh的值。原因是因为该ACK确认了新的数据，说明从重复ACK时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态。注意，如果在此过程出现超时，则重新进入慢启动阶段。

　　好了，讲了这么多，完全可以一图概之：

　　 ![img](https://img-blog.csdn.net/20150718104424646) 

参考：
http://baike.baidu.com/view/1199185.htm
http://www.knowsky.com/383954.html
http://blog.jobbole.com/71427/
http://blog.csdn.net/todd911/article/details/10026441
http://www.netis.com.cn/flows/2012/08/tcp-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E7%AE%80%E4%BB%8B/
http://www.firefoxbug.com/index.php/archives/2798/
http://coolshell.cn/articles/11609.html
http://www.ccs-labs.org/teaching/rn/animations/gbn_sr/
http://www.cnblogs.com/fll/archive/2008/06/10/1217013.html
http://blog.csdn.net/todd911/article/details/10026441



## 9，TCP拥塞算法:BBR算法与Reno/CUBIC的对比

 https://blog.csdn.net/wangdd_199326/article/details/79022782 



我一再强调，BBR算法是个分界点，所有的TCP拥塞控制算法，被分为BBR之前和BBR之后的(其实发现，这并不是我个人的观点，很多人都这么认为，所有想写本文探个究竟)。当然这里的”所有“并不包括封闭的那些算法，比如垃圾公司Appex的算法，或者伟大的垃圾微软的算法。任何的算法都内含了一个进化的过程，CUBIC和Reno看起来非常不同，但是却属于同一个思想，因此可以说，CUBIC是Reno的高级版本**(将一种锯齿换成另一种锯齿而已)**，事实上，TCP拥塞控制算法一直处在不断的改进之中，起初，Reno算法，然后就是NewReno，再往后就是各种混战，直到最终落实到了CUBIC，起码在Linux平台是这么一个过程，其它的平台，也是大同小异。
        关于从Reno到CUBIC进化过程的细节，这里不谈，相信80%的人可能并不在乎，另外20%在乎的人可能比我懂得更多，所以，这里不谈。
        ...[但是我还是要把篇幅留下来]
        这里要谈的是BBR和Reno/CUBIC的一致性，这听起来可能让人觉得惊奇，它们不是有本质的区别吗？为什么要说它们的一致性呢？
        我特别感谢Van Jacobson这位大师。虽然他不认识我，我也没见过他，但是他提出的问题导致了Reno，CUBIC直到BBR的诞生，我估计Appex和微软这种只吃饭不拉屎的貔貅也是借鉴了他的思想。
        首先要声明，我并不是Google的粉丝，所以我更能采用公正的眼光看待BBR算法，虽然我一再强调它的多么多么的创新，但是这并不意味着BBR就是最好的，如果把BBR作为一个起点，类似Reno那样的起点，BBR本身也会有一个进化的过程，我希望的是大家都参与这个过程，我能做的只是抛砖引玉。我只是一个宣传者，一个鼓手。顺便说一句，我比较崇拜写出《大教堂与集市》并且会吹笛子的Eric S· Raymond，他自称自己是一名鼓手(为什么不是笛子手呢？)。
        我们先看一下Van Jacobson提出的一些的问题。
**To achieve network stability, TCP entity should obey a ‘packet conservation’ principle, which is borrowed from physics.**

**What is packet conservation principle?**
**--A new packet is not put into the network until an old packet leaves( i.e. ‘conservation’) while maintaining ‘equilibrium’ in which a connection runs stably**
**by effectively using the bandwidth available on the path.**

OK，理解这个是简单且直观的，然后呢？VJ继续说：
**Three failures of packet conservation**

  1. **The connection does not get to equilibrium**
  2. **A sender injects a new packet before an old packet**
      **has exited**
  3. **The equilibrium can’t be reached because of  
      resource limits along the path**

     **1, 3 : Equilibrium problem     2 : Conservation problem**

**To fix those failures, three algorithm are introduced**
     *** Slow-start algorithm for 1.**
     *** Retransmit timeout estimation algorithm for 2.**
     *** Congestion avoidance algorithm for 3.**
OK，问题提过了，方案也有了，接下来就是如何解决问题了，也就是说，如何把方案变成代码。
我们看一张图，一张到目前为止，BBR算法之前的TCP拥塞控制算法的图解，假设你对TCP拥塞算法有所了解那么这个图就是直观的：



 ![img](https://img-blog.csdn.net/20161029071122388?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 


当然，这里并不包括快速重传/快速恢复算法。为什么？难道快速恢复不重要吗？不是不重要，而是，快速恢复属于上述经典图解的一个优化！我不是说过吗，算法是一个进化的过程，不管最终变得多么复杂，其根本是不会变的。在BBR之前，其根本就是上图所示。

        按照VJ的想法，TCP拥塞控制的根本就是实现上图中红色框框所框住的那些阶段的处理逻辑。我们先看看BBR之前是怎么处理的，这虽然并不重要，但看看毕竟不多：
```JAVA
cwnd = 1;
while(1) {
     send packets of min (cwnd, rwnd);                 // burst
     wait until receiving all ACKs for the previous sent packets         slow-start
     if ( timeout occurs )  break;      else   cwnd = 2*cwnd;
}
threshold = cwnd/2;    cwnd = 1;
while(1){
    if ( cwnd < threshold ){
        send packets of min (cwnd, rwnd);              //burst
        wait until receiving all ACKs for the previous sent packets      slow-start
        if ( timeout occurs ){ threshold = cwnd/2;    cwnd = 1;}
        else   cwnd = 2*cwnd;}
        if ( cwnd == threshold || cwnd > threshold )
            send packets of min (threshold, rwnd);     //burst
    } else if ( cwnd > threshold ){
        send a new packet whenever an ACK is received    //self-clocking
        if ( Sender receives all ACKs for the previous sent packets )
            {cwnd = cwnd + 1; send a new packet;}   //to probe more bandwidth
        if ( timeout occurs)
            {threshold = cwnd/2;    cwnd = 1;}          //to avoid congestion
    }
}
```



然而，还是VJ的话：
**“The way we’ve always done it” is not necessarily the same as “the right way to do it”**
太TMD的经典了！
然后，我们看下BBR是怎么做的。

 ![img](https://img-blog.csdn.net/20161029071150629?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 



通过研究上面两个图，你能看出什么呢？
**Reno/CUBIC：**
它们是事件驱动的！无论是丢包，还是RTT增加，都被视为一种严重的事件，这些严重的事件导致TCP拥塞控制系统在”To find current bandwidth“，”To avoid congestion“以及
”To probe more bandwidth“之间切换，最终落实下来的就是促使拥塞算法(无论Reno，Vegas还是CUBIC)调整窗口的大小。
        那么谁来发现这些事件是否发生呢？答案是TCP拥塞控制状态机。拥塞控制状态机直接主导这些状态的切换，只要它发现丢包，不管是不是真的，都会拉低窗口值。
        所以说，Reno/CUBIC的窗口调整是被动的。
**BBR：**
bbr是反馈驱动的！bbr内置了自主的调速机制，不受TCP拥塞控制状态机的控制，bbr算法是自闭的，它可以自己完成VJ的所有状态探测以及切换，无需外界干涉，且对外界的干涉视而不见。
        bbr周期性的试图探测是否有更多的带宽，如果有，那么就利用它，如果没有，就退回到之前的状态。
        所以说，bbr的窗口调整是主动的。

当你看到Reno/CUBIC和bbr的区别的时候，可能会想起中断和轮询的区别，bbr和轮询之间的不同点是，bbr有事实的反馈。



好了，这就是Reno/CUBIC和bbr的区别，它们同样完成了”To find current bandwidth“，”To avoid congestion“以及”To probe more bandwidth“的逻辑，只是一个是事件驱动的被动实现，一个是反馈驱动的主动实现。和同事聊天时，在聊到高血压的时候，有一个很类似的事实：有一些人只有当觉得自己头晕的时候才会采取降血压的措施，另一些人则不断喝水并且少烟酒让自己血压维持在正常水平...



本文的最后，我们来看一下bbr算法自身的一些话题。
事实上，早在1981年的时候Gail & Kleinrock就发现了bbr采用的模型：



**Rather than using events such as loss or buffer occupancy, which are only weakly correlated with congestion, BBR starts from Kleinrock’s formal model of  congestion and its associated optimal operating point.**

然而，在当时和后续30年中，人们认为这是一件比较困难的事情，你怎么知道当前的带宽是不是最大的且RTT是最小的，要知道，带宽和RTT是相关的，它们的乘积就是BDP！直到最近，Google的bbr才意识到了一种非常简单的方法：
**While it is impossible to disambiguate any single bandwidth or RTT measurement, a connection's behavior over time tells a clearer story.**
有多么clearer呢？
bbr采用了一个时间窗口内的带宽测量和RTT测量，关键是这句话里面的”over time“！bbr算法之所以可以完成最大带宽和最小RTT的测量，关键在于以下的点：

 ![img](https://img-blog.csdn.net/20161029071216592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 



当到达最大带宽的时候，RTT开始增长(所有不以增加速率为目的的缓存都是耍流氓！缓存不直接增加速率，缓存通过降低丢包来提高网络利用率！)，无论从RTT视图还是从带宽视图来看，两个操作点都是一致的，这是基本！那么bbr是怎么测量的呢？
        **先稳定住带宽，当带宽不再增加的时候，bbr认为已经达到最大带宽可，然后在此基础上测量RTT，带宽的RTT的操作点是重合的！就是这么简单且直接。**



 现在，你已经明白了，无论任何TCP拥塞控制算法，解决的都是一个问题(这个结论太简单了，简直就是废话)。这是方法不同而已。

    最后，附上一些话。
    除了算法级的优化，还有实现级的优化，比如如何利用当今的多核处理器，如何优化中断，如何优化锁...VJ曾经说过，现如今的TCP实现基本都是沿袭自BSD，而BSD又直接沿袭自Multics，然而Multics根本就是个古董。所以VJ说人们一贯的做法并不一定是正确的做法。回到bbr，作为TCP/IP协议最初的起草人之一，虽然VJ并不是bbr算法的直接实现者，但是VJ贡献的东西可能比任何其它人都要多，都要强。
    


## 10，TCP三次握手数据丢失会发生什么

#### 10.1	TCP收到RST的几种情况

在某些特殊情况下，**TCP连接的一端会向另一端发送复位报文段，以通知对方关闭或重新建立连接。**

一般来说，**有以下三种情况：**

1. **访问不存在的端口**。

   ​	若端口不存，则直接返回RST，同时RST报文接收通告窗口大小为0.

   其实客户端向服务器的某个端口发起连接，如果端口被处于TIME_WAIT 状态的连接占用时，客户端也会收到RST

2. **异常终止连接。**        一方直接发送RST报文，表示异常终止连接。一旦发送方发送复位报文段，发送端所有排队等待发送的数据都被丢弃。应用程序可以通过socket选项                                                SO_LINGER来发送RST复位报文。

**3.处理半打开连接**。      一方关闭了连接，另一方却没有收到结束报文（如网络故障），此时另一方还维持着原来的连接。而一方即使重启，也没有该连接的任何信息。这种状态                                        就叫做半打开连接。而此时另一方往处于半打开状态的连接写数据，则对方回应RST复位报文。

#### 10.2	TCP三次握手及相关面试题

#### 一、TCP三次握手过程

TCP（Transmission Control Protocol）传输控制协议，提供可靠的面向连接的服务，采用三次握手建立连接。

##### 1.名词解读

**SYN**：同步序列号，用来发起一个连接。SYN=1的报文不能携带数据，但是消耗掉一个序号。

**ACK**：确认标识，当ACK=1时确认字段才有效。

**seq**：序列号，即数据包本身的序列号，为连接以后传输数据使用；

**ack**：对收到数据包的确认，值是等待接受的数据包的序列号（即期望对方继续发送的那个数据包的序列号）。

##### 2.TCP三次握手过程

如下图为三次握手的过程。

 ![img](https://img-blog.csdn.net/20180401211718518?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BlaXBlaWx1bw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 

**第一次**：客户端C向服务端S发送连接请求报文，该报文首部中的SYN=1，ACK=0，随机选取一个序列号seq=i作为初始序列号。此时，客户端进入SYN_SEND同步已发送状态。

**第二次**：服务端收到客户端的连接请求报文，如果同意建立连接，则发送确认报文。确认报文首部中SYN=1、ACK=1、ack=i+1、seq=j（服务端的初始序列号）。此时，服务器进入SYN_RCVD同步收到状态。

**注：为什么ack=i+1？**

答：服务器对客户端的数据进行确认，因为已经收到序列号为i的数据包，准备接受序列号为i+1的数据包，所以确认号ack=i+1。

**第三次**：客户端收到服务端的确认报文之后，会向服务器发送确认报文，告诉服务器收到了它的确认报文并准备建立连接。确认报文首部中SYN=0、ACK=1、ack=j+1、seq=i+1。服务端收到客户端确认报文，此时，服务端进入ESTABLISHED已建立连接状态。

**注：为什么seq=i+1?**

答：ACK报文段可以携带数据，因此如果不携带数据，不消耗序列号，则下一个报文的序列号仍然是seq=i+1；如果携带数据，则序列号为在i+1的基础上增加携带数据的大小。此处默认第三次握手客户端不发送携带数据的报文段。

#### 二、TCP为什么采用三次握手，而不是两次握手或四次握手？

##### 1.TCP三次握手理论背景

(a)TCP提供可靠的面向连接的服务，TCP是全双工的，即任意一端可以发送数据，也可以接受数据，需要有一个发送序列号和接受序列号。

(b)TCP三次握手的目的是同步连接双方的序列号和确认号，并交换TCP窗口大小信息，确认双方有收发数据的能力。

##### 2.为什么不是两次握手或四次握手？

防止失效的连接请求报文段被服务端接受，从而产生错误。

(a)如果建立连接需要两次握手，漫画图解：

 ![img](https://img-blog.csdn.net/20180402144228180?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BlaXBlaWx1bw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 

> 解读：
>
> 皇上让A小栗子宣爱妃，A由于事情耽误没及时赶到宣口谕；
>
> 皇上坐等右等，派出B小二去，并成功宣明了口谕；
>
> 爱妃收到B小二的口谕，赶来陪皇上解闷儿；
>
> 连接成功建立，等释放连接之后，A小栗子此时才姗姗来迟，宣明口谕，爱妃以为皇上独宠于己身，打扮花枝招展前来，由于旨意已失效，连接不存在，找不到是谁人宣，就一直等啊等啊等..........嘎嘎



两次握手，客户端收到服务端的应答后进入ESTABLISHED（已建立连接状态），而服务端在收到客户端的连接请求之后就进入了ESTABLISHED状态。如果出现网络拥塞，客户端发送的连接请求报文A过了很久没有到达服务端，会超时重发请求报文B，服务端正确接受并确认应答，连接建立并开始通信传输数据，等通信结束之后释放连接。此时，如果之前失效的连接请求A到达服务端，由于两次握手就能成功建立连接，服务端收到请求A之后进入ESTABLISHED已建立连接状态，等待发送数据或者主动发送数据，此时，客户端已经进入CLISED断开连接状态，服务器会一直等下去，浪费服务器连接资源。

**(b)建立连接需要四次握手**

由于三次握手已经能确保建立可靠的连接，所以不需要四次或更多的握手。

参考：https://blog.csdn.net/hacker00011000/article/details/52319111        



## 11，TCP/UDP报最大长度

#### 11.1	TCP/UDP报文格式

- [TCP/UDP 报最大长度](https://blog.csdn.net/cui918/article/details/53286323)
- [据说,UDP传输最好不要超过1024 也就是1K,有这个事情吗?](https://bbs.csdn.net/topics/210084346)

#### 11.2TCP/UDP优缺点以及区别

**TCP的优点**： 可靠，稳定 TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源。 TCP的缺点： 慢，效率低，占用系统资源高，易被攻击 TCP在传递数据之前，要先建连接，这会消耗时间，而且在数据传递时，确认机制、重传机制、拥塞控制机制等都会消耗大量的时间，而且要在每台设备上维护所有的传输连接，事实上，每个连接都会占用系统的CPU、内存等硬件资源。 而且，因为TCP有确认机制、三次握手机制，这些也导致TCP容易被人利用，实现DOS、DDOS、CC等攻击。

**UDP的优点**： 快，比TCP稍安全 UDP没有TCP的握手、确认、窗口、重传、拥塞控制等机制，UDP是一个无状态的传输协议，所以它在传递数据时非常快。没有TCP的这些机制，UDP较TCP被攻击者利用的漏洞就要少一些。但UDP也是无法避免攻击的，比如：UDP Flood攻击…… UDP的缺点： 不可靠，不稳定 因为UDP没有TCP那些可靠的机制，在数据传递时，如果网络质量不好，就会很容易丢包。 基于上面的优缺点，那么： 什么时候应该使用TCP： 当对网络通讯质量有要求的时候，比如：整个数据要准确无误的传递给对方，这往往用于一些要求可靠的应用，比如HTTP、HTTPS、FTP等传输文件的协议，POP、SMTP等邮件传输的协议。 在日常生活中，常见使用TCP协议的应用如下： 浏览器，用的HTTP FlashFXP，用的FTP Outlook，用的POP、SMTP Putty，用的Telnet、SSH QQ文件传输 ………… 什么时候应该使用UDP： 当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP。 比如，日常生活中，常见使用UDP协议的应用如下： QQ语音 QQ视频 TFTP ……

有些应用场景对可靠性要求不高会用到UPD，比如长视频，要求速率

**小结TCP与UDP的区别：**



1.基于连接与无连接；
2.对系统资源的要求（TCP较多，UDP少）；
3.UDP程序结构较简单；
4.流模式与数据报模式 ；

5.TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证。



```
tcp协议和udp协议的差别 TCP UDP 是否连接 面向连接 面向非连接 传输可靠性 可靠 不可靠 应用场合 传输大量数据 少量数据 速度 慢 快
```

TCP与UDP区别总结：

1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接

2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付
3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的
UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）
4、每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信
5、TCP首部开销20字节;UDP的首部开销小，只有8个字节

6、TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道

7、TCP对会消耗系统资源，而UDP相对而言不会消耗太多

 https://blog.csdn.net/zhang6223284/article/details/81414149 

HTTP与TCP的区别和联系

#### 1、TCP连接

   手机能够使用联网功能是因为手机底层实现了TCP/IP协议，可以使手机终端通过无线网络建立TCP连接。TCP协议可以对上层网络提供接口，使上层网络数据的传输建立在“无差别”的网络之上。

   建立起一个TCP连接需要经过“三次握手”：

   第一次握手：客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认；

   第二次握手：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；

   第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。

   握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连 接之前，TCP 连接都将被一直保持下去。断开连接时服务器和客户端均可以主动发起断开TCP连接的请求，断开过程需要经过“四次握手”（过程就不细写 了，就是服务器和客户端交互，最终确定断开）



#### 2、HTTP连接

   HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。

   HTTP连接最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。

   1）在HTTP 1.0中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。

   2）在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。

   由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。通常的 做法是即时不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道客 户端“在线”。若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。



3、SOCKET原理

3.1套接字（socket）概念

   套接字（socket）是通信的基石，是支持TCP/IP协议的网络通信的基本操作单元。它是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。

   应用层通过传输层进行数据通信时，TCP会遇到同时为多个应用程序进程提供并发服务的问题。多个TCP连接或多个应用程序进程可能需要通过同一个 TCP协 议端口传输数据。为了区别不同的应用程序进程和连接，许多计算机[操作系统](http://www.myhack58.com/Article/48/Article_048_1.htm)为应用程序与TCP／IP协议交互提供了套接字(Socket)接口。应用层可以 和传输层通过Socket接口，区分来自不同应用程序进程或网络连接的通信，实现数据传输的并发服务。


3.2 建立socket连接


   建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。

   套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。

   服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。

   客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。

   连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户 端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。


4、SOCKET连接与TCP连接

   创建Socket连接时，可以指定使用的传输层协议，Socket可以支持不同的传输层协议（TCP或UDP），当使用TCP协议进行连接时，该Socket连接就是一个TCP连接。


5、Socket连接与HTTP连接

   由于通常情况下Socket连接就是TCP连接，因此Socket连接一旦建立，通信双方即可开始相互发送数据内容，直到双方连接断开。但在实际网络应用 中，客户端到服务器之间的通信往往需要穿越多个中间节点，例如路由器、网关、防火墙等，大部分防火墙默认会关闭长时间处于非活跃状态的连接而导 致 Socket 连接断连，因此需要通过轮询告诉网络，该连接处于活跃状态。

   而HTTP连接使用的是“请求—响应”的方式，不仅在请求时需要先建立连接，而且需要客户端向服务器发出请求后，服务器端才能回复数据。

   很多情况下，需要服务器端主动向客户端推送数据，保持客户端与服务器数据的实时与同步。此时若双方建立的是Socket连接，服务器就可以直接将数据传送给 客户端；若双方建立的是HTTP连接，则服务器需要等到客户端发送一次请求后才能将数据传回给客户端，因此，客户端定时向服务器端发送连接请求，不仅可以 保持在线，同时也是在“询问”服务器是否有新的数据，如果有就将数据传给客户端。

 

二、相互关系

   首先，纠正一下我以前一直误解的概念，我一直以为Http和Tcp是两种不同的，但是地位对等的协议，虽然知道TCP是传输层，而http是应用层今天学习了下，知道了 http是要基于TCP连接基础上的，简单的说，TCP就是单纯建立连接，不涉及任何我们需要请求的实际数据，简单的传输。http是用来收发数据，即实际应用上来的。

   第一：从传输层，先说下TCP连接，我们要和服务端连接TCP连接，需要通过三次连接，包括：请求，确认，建立连接。即传说中的“三次握手协议”。

   第一次：C发送一个请求连接的位码SYN和一个随机产生的序列号给Seq，然后S收到了这些数据。

   第二次:S收到了这个请求连接的位码，啊呀，有人向我发出请求了么，那我要不要接受他的请求，得实现确认一下，于是，发送了一个确认码 ACN（seq+1），和SYN，Seq给C，然后C收到了，这个是第二次连接。

   第三次：C收到了确认的码和之前发送的SYN一比较，偶哟，对上了么，于是他又发送了一个ACN（SEQ+1）给S，S收到以后就确定建立连接，至此，TCP连接建立完成。

   简单就是：请求，确认，连接。

 

   第二：从实际上的数据应用来说httP

   在前面客户端和应用服务器建立TCP连接之后，就需要用http协议来传送数据了，HTTP协议简单来说，还是请求，确认，连接。

   总体就是C发送一个HTTP请求给S，S收到了这个http请求，然后返回给Chttp响应，然后C的中间件或者说浏览器把这些数据渲染成为了网页，展示在用户面前。

   第一：发送一个http请求给S，这个请求包括请求头和请求内容：

request header：

   包括了，1.请求的方法是POST/GET,请求的URL，http协议版本2.请求的数据，和编码方式3是否有cookie和cooies，是否缓存等。

   post和get请求方式的区别是，get把请求内容放在URL后面，但是URL长度有限制。而post是以表单的形势，适合要输入密码之类的，因为不在URL中显示，所以比较安全。

request body：

即请求的内容.

   第二：S收到了http请求，然后根据请求头，返回http响应。

response header：包括了1.cookies或者sessions2.状态吗3.内容大小等

response body：

   即响应的内容，包括，JS什么的。

   第三，C收到了以后，就由浏览器完成一系列的渲染，包括执行JS脚本等。

   这就是我所理解的webTCP,HTTP基础知识，待续。。。。。

 

   TCP是底层通讯协议，定义的是数据传输和连接方式的规范
   HTTP是应用层协议，定义的是传输数据的内容的规范
   HTTP协议中的数据是利用TCP协议传输的，所以支持HTTP也就一定支持TCP   

   HTTP支持的是www服务 
   而TCP/IP是协议 
   它是Internet国际互联网络的基础。TCP/IP是网络中使用的基本的通信协议。 
   TCP/IP实际上是一组协议，它包括上百个各种功能的协议，如：远程登录、文件传输和电子邮件等，而TCP协议和IP协议是保证数据完整传输的两个基本的重要协议。通常说TCP/IP是Internet协议族，而不单单是TCP和IP。

## 12	http和https的区别与联系？



## 13 为什么说Http是无状态的协议？

- [http协议和web应用有状态和无状态辨析](http://blog.sina.com.cn/s/blog_93b45b0f0101a4ix.html)

## 14 Http的Http basic authentication

- [HTTP基本认证(Basic Authentication)](https://blog.csdn.net/wochunyang/article/details/78675325)



## 15.0	HTTP POST&GET

Http定义了与服务器交互的不同方法，最基本的方法有4种，分别是**GET，POST，PUT，DELETE**。URL全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而HTTP中的GET，POST，PUT，DELETE就对应着对这个资源的**查，改，增，删**4个操作。到这里，大家应该有个大概的了解了，GET一般用于获取/查询资源信息，而POST一般用于更新资源信息。

　　**1.根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的。**

　　(1).所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。

　　* 注意：这里安全的含义仅仅是指是非修改信息。

　　(2).幂等的意味着对同一URL的多个请求应该返回同样的结果。这里我再解释一下**幂等**这个概念：

> **幂等**（idempotent、idempotence）是一个数学或计算机学概念，常见于抽象代数中。
> 　　幂等有一下几种定义：
> 　　对于单目运算，如果一个运算对于在范围内的所有的一个数多次进行该运算所得的结果和进行一次该运算所得的结果是一样的，那么我们就称该运算是幂等的。比如绝对值运算就是一个例子，在实数集中，有abs(a)=abs(abs(a))。
> 　　对于双目运算，则要求当参与运算的两个值是等值的情况下，如果满足运算结果与参与运算的两个值相等，则称该运算幂等，如求两个数的最大值的函数，有在在实数集中幂等，即max(x,x) = x。

看完上述解释后，应该可以理解GET幂等的含义了。

　　但在实际应用中，以上2条规定并没有这么严格。引用别人文章的例子：比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。从根本上说，如果目标是当用户打开一个链接时，他可以确信从自身的角度来看没有改变资源即可。

　　**2.根据HTTP规范，POST表示可能修改变服务器上的资源的请求**。继续引用上面的例子：还是新闻以网站为例，读者对新闻发表自己的评论应该通过POST实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。

 

　　上面大概说了一下HTTP规范中GET和POST的一些原理性的问题。但在实际的做的时候，很多人却没有按照HTTP规范去做，导致这个问题的原因有很多，比如说：

　　**1**.很多人贪方便，更新资源时用了GET，因为用POST必须要到FORM（表单），这样会麻烦一点。

　　**2**.对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE。

　　**3**.另外一个是，早期的Web MVC框架设计者们并**没有有意识地将URL当作抽象的资源来看待和设计**，所以导致一个比较严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。

 　* 简单解释一下MVC：MVC本来是存在于Desktop程序中的，M是指数据模型，V是指用户界面，C则是控制器。使用MVC的目的是将M和V的实现代码分离，从而使同一个程序可以使用不同的表现形式。

　　以上3点典型地描述了老一套的风格（没有严格遵守HTTP规范），随着架构的发展，现在出现REST(Representational State Transfer)，一套支持HTTP规范的新风格，这里不多说了，可以参考《RESTful Web Services》。

 

　　说完原理性的问题，我们再**从表面现像上面看看GET和POST的区别：**

　　**1**.**GET请求的数据会附在URL之后**（就是把数据放置在HTTP协议头中），**以?分割URL和传输数据，参数之间以&相连**，如：login.action?name=hyddd&password=idontknow&verify=%E4%BD%A0%E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如：%E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。

　　**POST把提交的数据则放置在是HTTP包的包体中**。

　　**2**."**GET方式提交的数据最多只能是1024字节，理论上POST没有限制，可传较大量的数据**，IIS4中最大为80KB，IIS5中为100KB"？？！

　　以上这句是我从其他文章转过来的，**其实这样说是错误的，不准确的**：

　　(1).首先是"GET方式提交的数据最多只能是1024字节"，因为GET是通过URL提交数据，那么GET可提交的数据量就跟URL的长度有直接关系了。而实际上，**URL不存在参数上限的问题，HTTP协议规范没有对URL长度进行限制**。这个限制是特定的浏览器及服务器对它的限制。IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。

　　注意这是限制是整个URL长度，而不仅仅是你的参数值数据长度。[见参考资料5]

　　(2).理论上讲，**POST是没有大小限制的，HTTP协议规范也没有进行大小限制**，说“POST数据量存在80K/100K的大小限制”是不准确的，POST数据是没有限制的，起限制作用的是服务器的处理程序的处理能力。

　　对于ASP程序，Request对象处理每个表单域时存在100K的数据长度限制。但如果使用Request.BinaryRead则没有这个限制。

　　由这个延伸出去，对于IIS 6.0，微软出于安全考虑，加大了限制。我们还需要注意：

　　　　 1).IIS 6.0默认ASP POST数据量最大为200KB，每个表单域限制是100KB。
　　　　 2).IIS 6.0默认上传文件的最大大小是4MB。
　　　　 3).IIS 6.0默认最大请求头是16KB。
　　IIS 6.0之前没有这些限制。[见参考资料5]

　　所以上面的80K，100K可能只是默认值而已(注：关于IIS4和IIS5的参数，我还没有确认)，但肯定是可以自己设置的。由于每个版本的IIS对这些参数的默认值都不一样，具体请参考相关的IIS配置文档。

　　**3**.在ASP中，**服务端获取GET请求参数用Request.QueryString，获取POST请求参数用Request.Form**。在JSP中，用request.getParameter(\"XXXX\")来获取，虽然jsp中也有request.getQueryString()方法，但使用起来比较麻烦，比如：传一个test.jsp?name=hyddd&password=hyddd，用request.getQueryString()得到的是：name=hyddd&password=hyddd。在PHP中，可以用$_GET和$_POST分别获取GET和POST中的数据，而$_REQUEST则可以获取GET和POST两种请求中的数据。值得注意的是，JSP中使用request和PHP中使用$_REQUEST都会有隐患，这个下次再写个文章总结。

　　**4**.**POST的安全性要比GET的安全性高**。注意：这里所说的安全性和上面GET提到的“安全”不是同个概念。上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义，比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为(1)登录页面有可能被浏览器缓存，(2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了，除此之外，使用GET提交数据还可能会造成Cross-site request forgery攻击。

　　总结一下，**Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求**，在FORM（表单）中，Method默认为"GET"，实质上，GET和POST只是发送机制不同，并不是一个取一个发！

## 15 HTTP请求的GET与POST方式的区别

#### 什么是 HTTP？

**超文本传输协议（HTTP）的设计目的是保证客户机与服务器之间的通信**。

HTTP 的工作方式是**客户机与服务器之间的请求-应答协议**。

web 浏览器可能是客户端，而计算机上的网络应用程序也可能作为服务器端。

举例：客户端（浏览器）向服务器提交 HTTP 请求；服务器向客户端返回响应。响应包含关于请求的状态信息以及可能被请求的内容。



#### 两种 HTTP 请求方法：GET 和 POST

在客户机和服务器之间进行请求-响应时，两种最常被用到的方法是：GET 和 POST。

- *GET* - 从指定的资源请求数据。
- *POST* - 向指定的资源提交要被处理的数据



##### GET 方法

请注意，查询字符串（名称/值对）是在 GET 请求的 URL 中发送的：

```
/test/demo_form.asp?name1=value1&name2=value2
```

有关 GET 请求的其他一些注释：

- GET 请求可被缓存
- GET 请求保留在浏览器历史记录中
- GET 请求可被收藏为书签
- GET 请求不应在处理敏感数据时使用
- GET 请求有长度限制
- GET 请求只应当用于取回数据

##### POST 方法

请注意，查询字符串（名称/值对）是在 POST 请求的 HTTP 消息主体中发送的：

```
POST /test/demo_form.asp HTTP/1.1
Host: w3schools.com
name1=value1&name2=value2
```

有关 POST 请求的其他一些注释：

- POST 请求不会被缓存
- POST 请求不会保留在浏览器历史记录中
- POST 不能被收藏为书签
- POST 请求对数据长度没有要求



##### 比较 GET 与 POST

下面的表格比较了两种 HTTP 方法：GET 和 POST。

![image-20200211235413097](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211235413097.png)



##### 其他 HTTP 请求方法

下面的表格列出了其他一些 HTTP 请求方法：

![image-20200211235437347](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20200211235437347.png)

## 16 Http1.0、Http1.1、Http2.0的区别

**HTTP1.0 VS HTTP1.1**

- 长连接：

HTTP1.0需要使用keep-alive参数来告知服务器端要建立一个长连接，而HTTP1.1默认支持长连接（PersistentConnection）和请求的流水线（Pipelining）处理，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启Connection： keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。

- 缓存：

在HTTP1.0中主要使用header里的If-Modified-Since,Expires来做为缓存判断的标准，HTTP1.1则引入了更多的缓存控制策略例如Entity tag，If-Unmodified-Since, If-Match, If-None-Match等更多可供选择的缓存头来控制缓存策略



带宽优化及网络连接的使用：

- 状态码：

在HTTP1.1中新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除

- 带宽优化：

HTTP 1.1支持只发送header信息(不带任何body信息)，如果服务器认为客户端有权限请求服务器，则返回100，否则返回401。客户端如果接收到100，才开始把请求body发送到服务器。
这样当服务器返回401的时候，客户端就可以不用发送请求body了，节约了带宽。

- HOST:

Host头处理，在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。


**HTTP1.1 VS HTTP2.0**

- 多路复用：

在HTTP/1.1协议中，浏览器客户端在同一时间针对同一域名的请求有一定数据限制。超过限制数目的请求会被阻塞。
HTTP2.0使用了多路复用的技术，做到同一个连接并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级。
当然HTTP1.1也可以多建立几个TCP连接，来支持处理更多并发的请求，但是创建TCP连接本身也是有开销的。
TCP连接有一个预热和保护的过程，先检查数据是否传送成功，一旦成功过，则慢慢加大传输速度。因此对应瞬时并发的连接，服务器的响应就会变慢。所以最好能使用一个建立好的连接，并且这个连接可以支持瞬时并发的请求。

- 首部压缩：

HTTP1.1不支持header数据的压缩，HTTP2.0使用HPACK算法对header的数据进行压缩，这样数据体积小了，在网络上传输就会更快。

- 服务器推送：

当我们对支持HTTP2.0的web server请求数据的时候，服务器会顺便把一些客户端需要的资源一起推送到客户端，免得客户端再次创建连接发送请求到服务器端获取。这种方式非常合适加载静态资源。

## 17 Http安全性

#### Https 介绍

##### 什么是Https

HTTPS（全称：Hypertext Transfer Protocol over Secure Socket Layer），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL

##### Https的作用

- **内容加密** 建立一个信息安全通道，来保证数据传输的安全；
- **身份认证** 确认网站的真实性
- **数据完整性** 防止内容被第三方冒充或者篡改

##### Https的劣势

**对数据进行加解密决定了它比http慢**

> 需要进行非对称的加解密，且需要三次握手。首次连接比较慢点，当然现在也有很多的优化。 

出于安全考虑，浏览器不会在本地保存HTTPS缓存。实际上，只要在HTTP头中使用特定命令，HTTPS是可以缓存的。Firefox默认只在内存中缓存HTTPS。但是，只要头命令中有Cache-Control: Public，缓存就会被写到硬盘上。 IE只要http头允许就可以缓存https内容，缓存策略与是否使用HTTPS协议无关。

##### HTTPS和HTTP的区别

- https协议需要到CA申请证书，一般免费证书很少，需要交费。

- http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协议。

- http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。

- http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。

  > http默认使用80端口，https默认使用443端口

下面就是https的整个架构，现在的https基本都使用TSL了，因为更加安全，所以下图中的SSL应该换为**SSL/TSL。**

 ![Https](https://img-my.csdn.net/uploads/201205/21/1337566646_9367.png) 


下面就上图中的知识点进行一个大概的介绍。

#### 加解密相关知识

##### 对称加密

对称加密(也叫私钥加密)指加密和解密使用相同密钥的加密算法。有时又叫传统密码算法，就是加密密钥能够从解密密钥中推算出来，同时解密密钥也可以从加密密钥中推算出来。而在大多数的对称算法中，加密密钥和解密密钥是相同的，所以也称这种加密算法为秘密密钥算法或单密钥算法。

> 常见的对称加密有：DES（Data Encryption Standard）、AES（Advanced Encryption Standard）、RC4、IDEA 

##### 非对称加密

与对称加密算法不同，非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey）；并且加密密钥和解密密钥是成对出现的。非对称加密算法在加密和解密过程使用了不同的密钥，非对称加密也称为公钥加密，在密钥对中，其中一个密钥是对外公开的，所有人都可以获取到，称为公钥，其中一个密钥是不公开的称为私钥。

非对称加密算法对加密内容的长度有限制，不能超过公钥长度。比如现在常用的公钥长度是 2048 位，意味着待加密内容不能超过 256 个字节。

#### 摘要算法

数字摘要是采用单项Hash函数将需要加密的明文“摘要”成一串固定长度（128位）的密文，这一串密文又称为数字指纹，它有固定的长度，而且不同的明文摘要成密文，其结果总是不同的，而同样的明文其摘要必定一致。“数字摘要“是https能确保数据完整性和防篡改的根本原因。

#### 数字签名

数字签名技术就是对“非对称密钥加解密”和“数字摘要“两项技术的应用，它将摘要信息用发送者的私钥加密，与原文一起传送给接收者。接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，与解密的摘要信息对比。如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性。
数字签名的过程如下：
**明文 --> hash运算 --> 摘要 --> 私钥加密 --> 数字签名**

数字签名有两种功效：
一、能确定消息确实是由发送方签名并发出来的，因为别人假冒不了发送方的签名。
二、数字签名能确定消息的完整性。

> **注意：**
> 数字签名只能验证数据的完整性，数据本身是否加密不属于数字签名的控制范围

#### 数字证书

##### 为什么要有数字证书？

对于请求方来说，它怎么能确定它所得到的公钥一定是从目标主机那里发布的，而且没有被篡改过呢？亦或者请求的目标主机本本身就从事窃取用户信息的不正当行为呢？这时候，我们需要有一个权威的值得信赖的第三方机构(一般是由政府审核并授权的机构)来统一对外发放主机机构的公钥，只要请求方这种机构获取公钥，就避免了上述问题的发生。

##### 数字证书的颁发过程

用户首先产生自己的密钥对，并将公共密钥及部分个人身份信息传送给认证中心。认证中心在核实身份后，将执行一些必要的步骤，以确信请求确实由用户发送而来，然后，认证中心将发给用户一个数字证书，该证书内包含用户的个人信息和他的公钥信息，同时还附有认证中心的签名信息(根证书私钥签名)。用户就可以使用自己的数字证书进行相关的各种活动。数字证书由独立的证书发行机构发布，数字证书各不相同，每种证书可提供不同级别的可信度。

##### 证书包含哪些内容

- 证书颁发机构的名称
- 证书本身的数字签名
- 证书持有者公钥
- 证书签名用到的Hash算法
  

###### 验证证书的有效性

###### 浏览器默认都会内置CA根证书，其中根证书包含了CA的公钥

- 证书颁发的机构是伪造的：浏览器不认识，直接认为是危险证书

- 证书颁发的机构是确实存在的，于是根据CA名，找到对应内置的CA根证书、CA的公钥。用CA的公钥，对伪造的证书的摘要进行解密，发现解不了,认为是危险证书.

- 对于篡改的证书，使用CA的公钥对数字签名进行解密得到摘要A,然后再根据签名的Hash算法计算出证书的摘要B，对比A与B，若相等则正常，若不相等则是被篡改过的。

- 证书可在其过期前被吊销，通常情况是该证书的私钥已经失密。较新的浏览器如Chrome、Firefox、Opera和Internet Explorer都实现了在线证书状态协议（OCSP）以排除这种情形：浏览器将网站提供的证书的序列号通过OCSP发送给证书颁发机构，后者会告诉浏览器证书是否还是有效的。

  > 1、2点是对伪造证书进行的。3是对于篡改后的证书验证，4是对于过期失效的验证。

#### SSL 与 TLS

##### SSL (Secure Socket Layer，安全套接字层)

SSL为Netscape所研发，用以保障在Internet上数据传输之安全，利用数据加密(Encryption)技术，可确保数据在网络上之传输过程中不会被截取，当前为3。0版本。

SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。

##### TLS (Transport Layer Security，传输层安全协议)

用于两个应用程序之间提供保密性和数据完整性。
TLS 1.0是IETF（Internet Engineering Task Force，Internet工程任务组）制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1，它是写入了 RFC 的。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake）。较低的层为 TLS 记录协议，位于某个可靠的传输协议（例如 TCP）上面。

##### SSL/TLS协议作用：

- 认证用户和服务器，确保数据发送到正确的客户机和服务器；
- 加密数据以防止数据中途被窃取；
- 维护数据的完整性，确保数据在传输过程中不被改变。

##### TLS比SSL的优势

- 对于消息认证使用密钥散列法：TLS 使用“消息认证代码的密钥散列法”（HMAC），当记录在开放的网络（如因特网）上传送时，该代码确保记录不会被变更。SSLv3.0还提供键控消息认证，但HMAC比SSLv3.0使用的（消息认证代码）MAC 功能更安全。
- 增强的伪随机功能（PRF）：PRF生成密钥数据。在TLS中，HMAC定义PRF。PRF使用两种散列算法保证其安全性。如果任一算法暴露了，只要第二种算法未暴露，则数据仍然是安全的。
- 改进的已完成消息验证：TLS和SSLv3.0都对两个端点提供已完成的消息，该消息认证交换的消息没有被变更。然而，TLS将此已完成消息基于PRF和HMAC值之上，这也比SSLv3.0更安全。
- 一致证书处理：与SSLv3.0不同，TLS试图指定必须在TLS之间实现交换的证书类型。
- 特定警报消息：TLS提供更多的特定和附加警报，以指示任一会话端点检测到的问题。TLS还对何时应该发送某些警报进行记录。
  

##### SSL、TLS的握手过程

SSL与TLS握手整个过程如下图所示，下面会详细介绍每一步的具体内容：

###### 客户端首次发出请求

由于客户端(如浏览器)对一些加解密算法的支持程度不一样，但是在TLS协议传输过程中必须使用同一套加解密算法才能保证数据能够正常的加解密。在TLS握手阶段，客户端首先要告知服务端，自己支持哪些加密算法，所以客户端需要将本地支持的加密套件(Cipher Suite)的列表传送给服务端。除此之外，客户端还要产生一个随机数，这个随机数一方面需要在客户端保存，另一方面需要传送给服务端，客户端的随机数需要跟服务端产生的随机数结合起来产生后面要讲到的 Master Secret 。

客户端需要提供如下信息：
- 支持的协议版本，比如TLS 1.0版
- 一个客户端生成的随机数，稍后用于生成”对话密钥”
- 支持的加密方法，比如RSA公钥加密
- 支持的压缩方法

###### 服务端首次回应

服务端在接收到客户端的Client Hello之后，服务端需要确定加密协议的版本，以及加密的算法，然后也生成一个随机数，以及将自己的证书发送给客户端一并发送给客户端。这里的随机数是整个过程的第二个随机数

服务端需要提供的信息：
- 协议的版本
- 加密的算法
- 随机数
- 服务器证书

###### 客户端再次回应

客户端首先会对服务器下发的证书进行验证，验证通过之后，则会继续下面的操作，客户端再次产生一个随机数（第三个随机数），然后使用服务器证书中的公钥进行加密，以及放一个ChangeCipherSpec消息即编码改变的消息，还有整个前面所有消息的hash值，进行服务器验证，然后用新秘钥加密一段数据一并发送到服务器，确保正式通信前无误。
客户端使用前面的两个随机数以及刚刚新生成的新随机数，使用与服务器确定的加密算法，生成一个Session Secret。

>ChangeCipherSpec
>ChangeCipherSpec是一个独立的协议，体现在数据包中就是一个字节的数据，用于告知服务端，客户端已经切换到之前协商好的加密套件（Cipher Suite）的状态，准备使用之前协商好的加密套件加密数据并传输了。

###### 服务器再次响应

服务端在接收到客户端传过来的第三个随机数的 加密数据之后，使用私钥对这段加密数据进行解密，并对数据进行验证，也会使用跟客户端同样的方式生成秘钥，一切准备好之后，也会给客户端发送一个 ChangeCipherSpec，告知客户端已经切换到协商过的加密套件状态，准备使用加密套件和 Session Secret加密数据了。之后，服务端也会使用 Session Secret 加密一段 Finish 消息发送给客户端，以验证之前通过握手建立起来的加解密通道是否成功。

###### 之后的客户端与服务器间通信

确定秘钥之后，服务器与客户端之间就会通过商定的秘钥加密消息了，进行通讯了。整个握手过程也就基本完成了。

> 值得特别提出的是：
> SSL协议在握手阶段使用的是非对称加密，在传输阶段使用的是对称加密，也就是说在SSL上传送的数据是使用对称密钥加密的！因为非对称加密的速度缓慢，耗费资源。其实当客户端和主机使用非对称加密方式建立连接后，客户端和主机已经决定好了在传输过程使用的对称加密算法和关键的对称加密密钥，由于这个过程本身是安全可靠的，也即对称加密密钥是不可能被窃取盗用的，因此，保证了在传输过程中对数据进行对称加密也是安全可靠的，因为除了客户端和主机之外，不可能有第三方窃取并解密出对称加密密钥！如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（Premaster secret）能不能被破解。

###### 其他补充

对于非常重要的保密数据，服务端还需要对客户端进行验证，以保证数据传送给了安全的合法的客户端。服务端可以向客户端发出 Cerficate Request 消息，要求客户端发送证书对客户端的合法性进行验证。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供USB密钥，里面就包含了一张客户端证书。

PreMaster secret前两个字节是TLS的版本号，这是一个比较重要的用来核对握手数据的版本号，因为在Client Hello阶段，客户端会发送一份加密套件列表和当前支持的SSL/TLS的版本号给服务端，而且是使用明文传送的，如果握手的数据包被破解之后，攻击者很有可能串改数据包，选择一个安全性较低的加密套件和版本给服务端，从而对数据进行破解。所以，服务端需要对密文中解密出来对的PreMaster版本号跟之前Client Hello阶段的版本号进行对比，如果版本号变低，则说明被串改，则立即停止发送任何消息。

###### session的恢复

有两种方法可以恢复原来的session：一种叫做session ID，另一种叫做session ticket。

###### session ID

session ID的思想很简单，就是每一次对话都有一个编号（session ID）。如果对话中断，下次重连的时候，只要客户端给出这个编号，且服务器有这个编号的记录，双方就可以重新使用已有的”对话密钥”，而不必重新生成一把。

session ID是目前所有浏览器都支持的方法，但是它的缺点在于session ID往往只保留在一台服务器上。所以，如果客户端的请求发到另一台服务器，就无法恢复对话

###### session ticket

客户端发送一个服务器在上一次对话中发送过来的session ticket。这个session ticket是加密的，只有服务器才能解密，其中包括本次对话的主要信息，比如对话密钥和加密方法。当服务器收到session ticket以后，解密后就不必重新生成对话密钥了。

目前只有Firefox和Chrome浏览器支持。

##### 总结

https实际就是在TCP层与http层之间加入了SSL/TLS来为上层的安全保驾护航，主要用到对称加密、非对称加密、证书，等技术进行客户端与服务器的数据加密传输，最终达到保证整个通信的安全性。



## 18.0URL到页面加载

#### 第一步过程

- 首先，你得在浏览器里输入要网址:
  - 例如百度或者facebook。

#### 第二步过程

- 浏览器查找域名的IP地址（域名就是指输入的网址）
  - 浏览器缓存 – 浏览器会缓存DNS记录一段时间。 有趣的是，操作系统没有告诉浏览器储存DNS记录的时间，这样不同浏览器会储存个自固定的一个时间（2分钟到30分钟不等）。
  - 路由器缓存 – 接着，前面的查询请求发向路由器，它一般会有自己的DNS缓存。
  - ISP DNS 缓存 – 接下来要check的就是ISP缓存DNS的服务器。在这一般都能找到相应的缓存记录。
  - 递归搜索 – 你的ISP的DNS服务器从跟域名服务器开始进行递归搜索，从.com顶级域名服务器到Facebook的域名服务器。一般DNS服务器的缓存中会 有.com域名服务器中的域名，所以到顶级服务器的匹配过程不是那么必要了。

------

#### 第三步过程

- 在请求之前，需要浏览器与服务器建立连接（TCP或者UDP）
  - 与服务器建立连接时TCP属于安全的连接，需要三次握手，这里不在说明。
- 与服务器响应软件建立管道连接（socket）

------

#### 第四步过程

- 浏览器给web服务器发送一个HTTP请求
  - 因为像Facebook主页这样的动态页面，打开后在浏览器缓存中很快甚至马上就会过期，毫无疑问他们不能从中读取。
    所以，浏览器将把一下请求发送到Facebook所在的服务器：
  - 下面为整个一个请求

```js
GET HTTP://facebook.com/ HTTP/1.1
Accept: application/x-ms-application, image/jpeg, application/xaml+xml, [...]
User-Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; [...]
DontTrackMeHere: gzip, deflate
Connection: Keep-Alive
Host: facebook.com
Cookie: datr=1265876274-[...]; locale=en_US; lsd=WW[...]; c_user=2101[...]1234567
```

- 解读上面请求体
  - GET 这个请求定义了要读取的URL： “[HTTP://facebook.com/](http://facebook.com/)”。
  - Connection头要求服务器为了后边的请求不要关闭TCP连接。
  - 请求中也包含浏览器存储的该域名的cookies。可能你已经知道，在不同页面请求当中，cookies是与跟踪一个网站状态相匹配的键值。这样cookies会存储登录用户名，服务器分配的密码和一些用户设置等。Cookies会以文本文档形式存储在客户机里，每次请求时发送给服务器。
- 补充说明
  - 用来看原始HTTP请求及其相应的工具很多。作者比较喜欢使用fiddler，当然也有像FireBug这样其他的工具。这些软件在网站优 化时会帮上很大忙。
    除了获取请求，还有一种是发送请求，它常在提交表单用到。发送请求通过URL传递其参数(e.g.: [HTTP://robozzle.com/puzzle.aspx?id=85](http://robozzle.com/puzzle.aspx?id=85))。发送请求在请求正文头之后发送其参数。
  - 像“[HTTP://facebook.com/](http://facebook.com/)”中的斜杠是至关重要的。这种情况下，浏览器能安全的添加斜杠。而像“HTTP: //example.com/folderOrFile”这样的地址，因为浏览器不清楚folderOrFile到底是文件夹还是文件，所以不能自动添加 斜杠。这时，浏览器就不加斜杠直接访问地址，服务器会响应一个重定向，结果造成一次不必要的握手。

------

#### 第五步过程

##### 服务器“处理”请求

- 服务器接收到获取请求，然后处理并返回一个响应。
  这表面上看起来是一个顺向的任务，但其实这中间发生了很多有意思的东西- 就像作者博客这样简单的网站，何况像baidu那样访问量大的网站呢！
- Web 服务器软件web服务器软件（像IIS和阿帕奇）接收到HTTP请求，然后确定执行什么请求来处理它。**请求处理就 是一个能够读懂请求并且能生成HTML来进行响应的程序（像ASP.NET,PHP,RUBY…）**。
- 举 个最简单的例子，需求处理可以以映射网站地址结构的文件层次存储。像[HTTP://example.com/folder1/page1.aspx](http://example.com/folder1/page1.aspx)这个地 址会映射/httpdocs/folder1/page1.aspx这个文件。
- web服务器软件可以设置成为地址人工的对应请求处理，这样 page1.aspx的发布地址就可以是[HTTP://example.com/folder1/page1](http://example.com/folder1/page1)。* 请求处理请求处理阅读 请求及它的参数和cookies。它会读取也可能更新一些数据，并讲数据存储在服务器上。然后，需求处理会生成一个HTML响应。
- 所 有动态网站都面临一个有意思的难点 -如何存储数据。小网站一半都会有一个SQL数据库来存储数据，存储大量数据和/或访问量大的网站不得不找一些办法把数据库分配到多台机器上。解决方案有：sharding （基于主键值讲数据表分散到多个数据库中），复制，利用弱语义一致性的简化数据库。
  委托工作给批处理是一个廉价保持数据更新的技术。举例来讲，Fackbook得及时更新新闻feed，但数据支持下的“你可能认识的人”功能只需要每晚更新（作者猜测是这样的，改功能如何完善不得而知）。批处理作业更新会导致一些不太重要的数据陈旧，但能使数据更新耕作更快更简洁。

------

#### 第六步

- 服务器发回一个HTML响应.
- 下面就是一个响应头（响应头与body体分开的）
  HTTP/1.1 200 OK
  Cache-Control: private, no-store, no-cache, must-revalidate, post-check=0,
  pre-check=0
  Expires: Sat, 01 Jan 2000 00:00:00 GMT
  P3P: CP=”DSP LAW”
  Pragma: no-cache
  Content-Encoding: gzip
  Content-Type: text/html; charset=utf-8
  X-Cnection: close
  Transfer-Encoding: chunked
  Date: Fri, 12 Feb 2010 09:05:55 GMT
  2b3Tn@[…]
  整个响应大小为35kB，其中大部分在整理后以blob类型传输。

内容编码头告诉浏览器整个响应体用gzip算法进行压缩。解压blob块后，你可以看到如下期望的HTML：

“http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd”>

#### 第七步

- 浏览器开始显示HTML
  - 在浏览器没有完整接受全部HTML文档时，它就已经开始显示这个页面了

------

#### 第八步

- 浏览器显示HTML时，它会注意到需要获取其他地址内容的标签。这时，浏览器会发送一个获取请求来重新获得这些文件。
- 下面是几个我们访问facebook.com时需要重获取的几个URL



## 18 从输入URL到页面加载发生了什么？

问题：在浏览器中输入URL到整个页面显示在用户面前时这个过程中到底发生了什么。仔细思考这个问题，发现确实很深，这个过程涉及到的东西很多。

总体来说分为以下几个过程:

1. DNS解析
2. TCP连接
3. 发送HTTP请求
4. 服务器处理请求并返回HTTP报文
5. 浏览器解析渲染页面
6. 连接结束

#### **具体过程**





------



##### **1、DNS解析**

DNS解析的过程就是寻找哪台机器上有你需要资源的过程。当你在浏览器中输入一个地址时，例如www.baidu.com，其实不是百度网站真正意义上的地址。互联网上每一台计算机的唯一标识是它的IP地址，但是IP地址并不方便记忆。用户更喜欢用方便记忆的网址去寻找互联网上的其它计算机，也就是上面提到的百度的网址。所以互联网设计者需要在用户的方便性与可用性方面做一个权衡，这个权衡就是一个网址到IP地址的转换，这个过程就是DNS解析。它实际上充当了一个翻译的角色，实现了网址到IP地址的转换。网址到IP地址转换的过程是如何进行的?

###### **解析过程**

DNS解析是一个递归查询的过程。

![img](http://mmbiz.qpic.cn/mmbiz_png/UtWdDgynLdbeCggUzibWOnQ2vPtGHZIicbxNQ7wtMHsCiaiaLvq0E1yext6nic9Oq3w5ibL4iaPxvVnlS7O9ibzMEHyBlQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上述图片是查找www.google.com的IP地址过程。首先在本地域名服务器中查询IP地址，如果没有找到的情况下，本地域名服务器会向根域名服务器发送一个请求，如果根域名服务器也不存在该域名时，本地域名会向com顶级域名服务器发送一个请求，依次类推下去。直到最后本地域名服务器得到google的IP地址并把它缓存到本地，供下次查询使用。从上述过程中，可以看出网址的解析是一个从右向左的过程: com -> google.com -> www.google.com。但是你是否发现少了点什么，根域名服务器的解析过程呢？事实上，真正的网址是www.google.com.，并不是我多打了一个.，这个.对应的就是根域名服务器，默认情况下所有的网址的最后一位都是.，既然是默认情况下，为了方便用户，通常都会省略，浏览器在请求DNS的时候会自动加上，所有网址真正的解析过程为: . -> .com -> google.com. -> www.google.com.。

#### 

##### **2、DNS优化**

了解了DNS的过程，可以为我们带来哪些？上文中请求到google的IP地址时，经历了8个步骤，这个过程中存在多个请求(同时存在UDP和TCP请求，为什么有两种请求方式，请自行查找)。如果每次都经过这么多步骤，是否太耗时间？如何减少该过程的步骤呢？那就是DNS缓存。

###### **DNS缓存**

DNS存在着多级缓存，从离浏览器的距离排序的话，有以下几种: 浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。

- 在你的chrome浏览器中输入:chrome://dns/，你可以看到chrome浏览器的DNS缓存。
- 系统缓存主要存在/etc/hosts(Linux系统)中:

![img](http://mmbiz.qpic.cn/mmbiz_png/UtWdDgynLdbeCggUzibWOnQ2vPtGHZIicbwfEBJGOWNamokKr3xicIRRsBKIteGE9icI9ColRL5ia2UmauOdHoicCIsw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

###### **DNS负载均衡**

不知道大家有没有思考过一个问题: DNS返回的IP地址是否每次都一样？如果每次都一样是否说明你请求的资源都位于同一台机器上面，那么这台机器需要多高的性能和储存才能满足亿万请求呢？其实真实的互联网世界背后存在成千上百台服务器，大型的网站甚至更多。但是在用户的眼中，它需要的只是处理他的请求，哪台机器处理请求并不重要。DNS可以返回一个合适的机器的IP给用户，例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等，这种过程就是DNS负载均衡，又叫做DNS重定向。大家耳熟能详的CDN(Content Delivery Network)就是利用DNS的重定向技术，DNS服务器会返回一个跟用户最接近的点的IP地址给用户，CDN节点的服务器负责响应用户的请求，提供所需的内容。在这里打个免费的广告，我平时使用的比较多的是七牛云的CDN(免费)储存图片，作为我个人博客的图床使用。

##### **3、TCP连接**

HTTP协议是使用TCP作为其传输层协议的，当TCP出现瓶颈时，HTTP也会受到影响。但由于TCP优化这一块我平常接触的并不是很多，再加上大学时的计算机网络的基础基本上忘完，所以这一部分我也就不在这里分析了。

###### **HTTPS协议**

我不知道把HTTPS放在这个部分是否合适，但是放在这里好像又说的过去。HTTP报文是包裹在TCP报文中发送的，服务器端收到TCP报文时会解包提取出HTTP报文。但是这个过程中存在一定的风险，HTTP报文是明文，如果中间被截取的话会存在一些信息泄露的风险。那么在进入TCP报文之前对HTTP做一次加密就可以解决这个问题了。HTTPS协议的本质就是HTTP + SSL(or TLS)。在HTTP报文进入TCP报文之前，先使用SSL对HTTP报文进行加密。从网络的层级结构看它位于HTTP协议与TCP协议之间。

![img](http://mmbiz.qpic.cn/mmbiz_png/UtWdDgynLdbeCggUzibWOnQ2vPtGHZIicbzI4HjrZcabOsNYU0IddepGWf8D1X7EW0Mo5H1v7r7UmzGB2gxJyRtw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

###### **HTTPS过程**

HTTPS在传输数据之前需要客户端与服务器进行一个握手(TLS/SSL握手)，在握手过程中将确立双方加密传输数据的密码信息。TLS/SSL使用了非对称加密，对称加密以及hash等。具体过程请参考经典的阮一峰先生的博客TLS/SSL握手过程。
HTTPS相比于HTTP，虽然提供了安全保证，但是势必会带来一些时间上的损耗，如握手和加密等过程，是否使用HTTPS需要根据具体情况在安全和性能方面做出权衡。



##### **4、HTTP请求**

其实这部分又可以称为前端工程师眼中的HTTP，它主要发生在客户端。发送HTTP请求的过程就是构建HTTP请求报文并通过TCP协议中发送到服务器指定端口(HTTP协议80/8080, HTTPS协议443)。HTTP请求报文是由三部分组成: **请求行**, **请求报头**和**请求正文**。

###### **请求行**

格式如下:
`Method Request-URL HTTP-Version CRLF`

```
eg: GET index.html HTTP/1.1
```

常用的方法有: GET, POST, PUT, DELETE, OPTIONS, HEAD。

TODO：

- GET和POST有什么区别？

###### **请求报头**

请求报头允许客户端向服务器传递请求的附加信息和客户端自身的信息。
PS: 客户端不一定特指浏览器，有时候也可使用Linux下的CURL命令以及HTTP客户端测试工具等。
常见的请求报头有: Accept, Accept-Charset, Accept-Encoding, Accept-Language, Content-Type, Authorization, Cookie, User-Agent等。

 ![img](http://mmbiz.qpic.cn/mmbiz_png/UtWdDgynLdbeCggUzibWOnQ2vPtGHZIicbP3DUsyteiaNPPerPPJ8x5TxmXa9ZFqZMMc9axlEY57vlbQYGIb4xvow/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 

上图是使用Chrome开发者工具截取的对百度的HTTP请求以及响应报文，从图中可以看出，请求报头中使用了Accept, Accept-Encoding, Accept-Language, Cache-Control, Connection, Cookie等字段。Accept用于指定客户端用于接受哪些类型的信息，Accept-Encoding与Accept类似，它用于指定接受的编码方式。Connection设置为Keep-alive用于告诉客户端本次HTTP请求结束之后并不需要关闭TCP连接，这样可以使下次HTTP请求使用相同的TCP通道，节省TCP连接建立的时间。

###### **请求正文**

当使用POST, PUT等方法时，通常需要客户端向服务器传递数据。这些数据就储存在请求正文中。在请求包头中有一些与请求正文相关的信息，例如: 现在的Web应用通常采用Rest架构，请求的数据格式一般为json。这时就需要设置Content-Type: application/json。

##### **5、服务器处理请求并返回HTTP报文**

自然而然这部分对应的就是后端工程师眼中的HTTP。后端从在固定的端口接收到TCP报文开始，这一部分对应于编程语言中的socket。它会对TCP连接进行处理，对HTTP协议进行解析，并按照报文格式进一步封装成HTTP Request对象，供上层使用。这一部分工作一般是由Web服务器去进行，我使用过的Web服务器有Tomcat, Jetty和Netty等等。

HTTP响应报文也是由三部分组成: **状态码**, **响应报头**和**响应报文**。

###### **状态码**

状态码是由3位数组成，第一个数字定义了响应的类别，且有五种可能取值:

- 1xx：指示信息–表示请求已接收，继续处理。
- 2xx：成功–表示请求已被成功接收、理解、接受。
- 3xx：重定向–要完成请求必须进行更进一步的操作。
- 4xx：客户端错误–请求有语法错误或请求无法实现。
- 5xx：服务器端错误–服务器未能实现合法的请求。平时遇到比较常见的状态码有:200, 204, 301, 302, 304, 400, 401, 403, 404, 422, 500(分别表示什么请自行查找)。

TODO:

- 301和302有什么区别？
- HTTP缓存

![img](http://mmbiz.qpic.cn/mmbiz_png/UtWdDgynLdbeCggUzibWOnQ2vPtGHZIicb5FbghY7Uq06wRx8esHodImdJEdMC2MnmGpcQfsl1LzIZib4Y4R50kTw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

该图是本公司对状态码的一个总结，绘制而成的status code map，请大家参考。

###### **响应报头**

常见的响应报头字段有: Server, Connection...。

###### **响应报文**

服务器返回给浏览器的文本信息，通常HTML, CSS, JS, 图片等文件就放在这一部分。

##### **6、浏览器解析渲染页面**

浏览器在收到HTML,CSS,JS文件后，它是如何把页面呈现到屏幕上的？下图对应的就是WebKit渲染的过程。

![img](http://mmbiz.qpic.cn/mmbiz_png/UtWdDgynLdbeCggUzibWOnQ2vPtGHZIicbyzQ4WPoxgXeLnXOYib27a2R5DpKCwibO5OjUUstfqPAdNn7e7WiaSDBxQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
浏览器是一个边解析边渲染的过程。首先浏览器解析HTML文件构建DOM树，然后解析CSS文件构建渲染树，等到渲染树构建完成后，浏览器开始布局渲染树并将其绘制到屏幕上。这个过程比较复杂，涉及到两个概念: reflow(回流)和repain(重绘)。DOM节点中的各个元素都是以盒模型的形式存在，这些都需要浏览器去计算其位置和大小等，这个过程称为relow;当盒模型的位置,大小以及其他属性，如颜色,字体,等确定下来之后，浏览器便开始绘制内容，这个过程称为repain。页面在首次加载时必然会经历reflow和repain。reflow和repain过程是非常消耗性能的，尤其是在移动设备上，它会破坏用户体验，有时会造成页面卡顿。所以我们应该尽可能少的减少reflow和repain。

![img](http://mmbiz.qpic.cn/mmbiz_png/UtWdDgynLdbeCggUzibWOnQ2vPtGHZIicbtAbEeGgibfejicg1rZK4bVwpdwlyZOqTUyhYJqlk9ce1Zniab0dYWyI5A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

JS的解析是由浏览器中的JS解析引擎完成的。JS是单线程运行，也就是说，在同一个时间内只能做一件事，所有的任务都需要排队，前一个任务结束，后一个任务才能开始。但是又存在某些任务比较耗时，如IO读写等，所以需要一种机制可以先执行排在后面的任务，这就是：同步任务(synchronous)和异步任务(asynchronous)。JS的执行机制就可以看做是一个主线程加上一个任务队列(task queue)。同步任务就是放在主线程上执行的任务，异步任务是放在任务队列中的任务。所有的同步任务在主线程上执行，形成一个执行栈;异步任务有了运行结果就会在任务队列中放置一个事件；脚本运行时先依次运行执行栈，然后会从任务队列里提取事件，运行任务队列中的任务，这个过程是不断重复的，所以又叫做事件循环(Event loop)。

浏览器在解析过程中，如果遇到请求外部资源时，如图像,iconfont,JS等。浏览器将重复1-6过程下载该资源。请求过程是异步的，并不会影响HTML文档进行加载，但是当文档加载过程中遇到JS文件，HTML文档会挂起渲染过程，不仅要等到文档中JS文件加载完毕还要等待解析执行完毕，才会继续HTML的渲染过程。原因是因为JS有可能修改DOM结构，这就意味着JS执行完成前，后续所有资源的下载是没有必要的，这就是JS阻塞后续资源下载的根本原因。CSS文件的加载不影响JS文件的加载，但是却影响JS文件的执行。JS代码执行前浏览器必须保证CSS文件已经下载并加载完毕。

##### **7、Web优化**

上面部分主要介绍了一次完整的请求对应的过程，了解该过程的目的无非就是为了Web优化。在谈到Web优化之前，我们回到一个更原始的问题，Web前端的本质是什么。我的理解是: 将信息快速并友好的展示给用户并能够与用户进行交互。快速的意思就是在尽可能短的时间内完成页面的加载，试想一下当你在淘宝购买东西的时候，淘宝页面加载了10几秒才显示出物品，这个时候你还有心情去购买吗？怎么快速的完成页面的加载呢？优雅的学院派雅虎给出了常用的一些手段，也就是我们熟悉的雅虎34条军规。这34军规实际上就是围绕请求过程进行的一些优化方式。

如何尽快的加载资源？答案就是能不从网络中加载的资源就不从网络中加载，当我们合理使用缓存，将资源放在浏览器端，这是最快的方式。如果资源必须从网络中加载，则要考虑缩短连接时间，即DNS优化部分;减少响应内容大小，即对内容进行压缩。另一方面，如果加载的资源数比较少的话，也可以快速的响应用户。当资源到达浏览器之后，浏览器开始进行解析渲染，浏览器中最耗时的部分就是reflow，所以围绕这一部分就是考虑如何减少reflow的次数。

##### **8、总结**

写这篇文章真的非常纠结，前前后后断断续续写了两个星期，因为涉及到的东西比较多，再加上有些东西记忆的没有那么清晰了，所以不好下笔。所涉及到的大部分内容，也基本上是一笔带过，只是给读者一个浅显的认知，当遇到相关的问题时，知道如何去查询。大家可以当成一篇Web开发的科普类文章去阅读。

另外在这里为公司的产品打个广告，在Chrome store中搜索DHC，这是一款超级好用的Web客户端工具，囊括了很多的功能: 报文分析，API测试等等，可谓说是WEB工程师必备工具。

## 19	Http和Socket 优劣比较？



## 20	如何实现可靠的UDP




 https://blog.csdn.net/gettogetto/article/details/76736365 